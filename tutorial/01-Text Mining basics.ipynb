{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGRAMS :  basically a set of co-occuring words within a given window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "sent = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
    "sent_bigrams = bigrams(sent)\n",
    "sent_trigrams = trigrams(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'quick'),\n",
       " ('quick', 'brown'),\n",
       " ('brown', 'fox'),\n",
       " ('fox', 'jumps'),\n",
       " ('jumps', 'over'),\n",
       " ('over', 'the'),\n",
       " ('the', 'lazy'),\n",
       " ('lazy', 'dog')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(sent_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'quick', 'brown'),\n",
       " ('quick', 'brown', 'fox'),\n",
       " ('brown', 'fox', 'jumps'),\n",
       " ('fox', 'jumps', 'over'),\n",
       " ('jumps', 'over', 'the'),\n",
       " ('over', 'the', 'lazy'),\n",
       " ('the', 'lazy', 'dog')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sent_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords are words which do not contain important significance to be used in text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Usually these stopwords are filtered out because they return vast amount of unnecessary information\n",
    "stopWords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sometimes we dont need punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is easy to get our hands on millions  of words of text  What can we do with it '"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set of punctuation characters\n",
    "pn = set(string.punctuation + '—')\n",
    "\n",
    "#function to remove punctuation characters\n",
    "\n",
    "def unpunkt(input):\n",
    "    output = ''.join(ch for ch in input if ch not in pn)\n",
    "    return output\n",
    "\n",
    "\n",
    "#lets test it.\n",
    "\n",
    "\n",
    "punkttext= '''It is easy to get our hands on millions !!! of words of #text . What can we do with it? '''\n",
    "unpunkt(punkttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### When we sent_tokenize a string we produce a list (of sentences).\n",
    "### Then from that we can word_tokenize a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        tokens=[]\n",
    "        \n",
    "        \n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "\n",
    "\n",
    "                # If  stopword, ignore token and continue otherwise provide lemma\n",
    "                if token in stopWords:\n",
    "                    continue\n",
    "                else:\n",
    "                    tokens.append(wordnet_lemmatizer.lemmatize(token).lower())\n",
    "                \n",
    "            \n",
    "        return tokens\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text='''     WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped \n",
    "into sets of cognitive synonyms (synsets), each expressing a distinct concept. \n",
    "Synsets are interlinked by means of conceptual-semantic and lexical relations. \n",
    "The resulting network of meaningfully related words and concepts can be navigated with the browser. \n",
    "WordNet is also freely and publicly available for download. WordNet’s structure makes it \n",
    " a useful tool for computational linguistics and natural language processing.\n",
    "\n",
    "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. \n",
    "However, there are some important distinctions. First, WordNet interlinks not just word forms—strings \n",
    "of letters—but specific senses of words. As a result, words that are found in close proximity to one \n",
    "another in the network are semantically disambiguated. Second, WordNet labels the semantic \n",
    "relations among words, whereas the groupings of words in a thesaurus does not \n",
    "follow any explicit pattern other than meaning similarity '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordnet',\n",
       " 'large',\n",
       " 'lexical',\n",
       " 'database',\n",
       " 'english',\n",
       " 'nouns',\n",
       " 'verb',\n",
       " 'adjective',\n",
       " 'adverb',\n",
       " 'grouped',\n",
       " 'set',\n",
       " 'cognitive',\n",
       " 'synonym',\n",
       " 'synset',\n",
       " 'expressing',\n",
       " 'distinct',\n",
       " 'concept',\n",
       " 'synsets',\n",
       " 'interlinked',\n",
       " 'mean',\n",
       " 'conceptualsemantic',\n",
       " 'lexical',\n",
       " 'relation',\n",
       " 'the',\n",
       " 'resulting',\n",
       " 'network',\n",
       " 'meaningfully',\n",
       " 'related',\n",
       " 'word',\n",
       " 'concept',\n",
       " 'navigated',\n",
       " 'browser',\n",
       " 'wordnet',\n",
       " 'also',\n",
       " 'freely',\n",
       " 'publicly',\n",
       " 'available',\n",
       " 'download',\n",
       " 'wordnet',\n",
       " '’',\n",
       " 'structure',\n",
       " 'make',\n",
       " 'useful',\n",
       " 'tool',\n",
       " 'computational',\n",
       " 'linguistics',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'wordnet',\n",
       " 'superficially',\n",
       " 'resembles',\n",
       " 'thesaurus',\n",
       " 'group',\n",
       " 'word',\n",
       " 'together',\n",
       " 'based',\n",
       " 'meaning',\n",
       " 'however',\n",
       " 'important',\n",
       " 'distinction',\n",
       " 'first',\n",
       " 'wordnet',\n",
       " 'interlinks',\n",
       " 'word',\n",
       " 'formsstrings',\n",
       " 'lettersbut',\n",
       " 'specific',\n",
       " 'sens',\n",
       " 'word',\n",
       " 'as',\n",
       " 'result',\n",
       " 'word',\n",
       " 'found',\n",
       " 'close',\n",
       " 'proximity',\n",
       " 'one',\n",
       " 'another',\n",
       " 'network',\n",
       " 'semantically',\n",
       " 'disambiguated',\n",
       " 'second',\n",
       " 'wordnet',\n",
       " 'label',\n",
       " 'semantic',\n",
       " 'relation',\n",
       " 'among',\n",
       " 'word',\n",
       " 'whereas',\n",
       " 'grouping',\n",
       " 'word',\n",
       " 'thesaurus',\n",
       " 'follow',\n",
       " 'explicit',\n",
       " 'pattern',\n",
       " 'meaning',\n",
       " 'similarity']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenize(unpunkt(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "Regular expressions (called REs, or regexes, or regex patterns) are essentially a tiny, \n",
    "highly specialized programming language embedded inside Python and made available through the re module.\n",
    "Using this little language, you specify the rules for the set of possible strings that you want to match; \n",
    "this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. \n",
    "You can then ask questions such as:\n",
    "“Does this string match the pattern?”, or “Is there a match for the pattern anywhere in this string?”. \n",
    "You can also use REs to modify a string or to split it apart in various ways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "htmltext ='''   <div class=\"container\">\n",
    "      <h3 class=\"quotes__subtitle chapeau-title chapeau-title--primary\">Testimonials</h3>\n",
    "      <h2 class=\"u-text-center\">Don’t just take our word for it.</h2>\n",
    "    <div class=\"quotes__wrapper js-quotes-height\">\n",
    "      <div class=\"js-quote\" data-index=\"0\" style=\"\">\n",
    "        <div class=\"quotes__quote\">The courses are excellent—they meet a higher standard than other platforms.</div>\n",
    "        <div class=\"quotes__description\">Helge is a PhD. Candidate in sociology and is learning data science tools to augment his research.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"1\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">Every month I learn something new on DataCamp, and they are all things that are very useful in my job.</div>\n",
    "        <div class=\"quotes__description\">Godefroy Clair is a Data Scientist at Flylab, where he helps create software for autonomous drones.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"2\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">DataCamp was essential for getting my job, which was my break into data science.</div>\n",
    "        <div class=\"quotes__description\">Jamen Long used the skills he learned at DataCamp to land a full time job as a data scientist.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"3\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">Data science is the future, and it is better to be on the cutting edge than left behind.</div>\n",
    "        <div class=\"quotes__description\">Arnaud uses techniques he&#39;s learned on DataCamp every day in his work for the French government.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"4\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">If anything, I regret I didn&#39;t start with DataCamp earlier.</div>\n",
    "        <div class=\"quotes__description\">Brigham Young University uses DataCamp to help students grasp data science concepts.</div>\n",
    "      </div>\n",
    "    </div>\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science',\n",
       " 'Data Scientist',\n",
       " 'data science',\n",
       " 'data scientist',\n",
       " 'Data science',\n",
       " 'data science']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using this little language, you specify the rules for the set of possible strings that you want to match; \n",
    "# i want to find all the strings that:\n",
    "# start either with capital or lower d then include 'ata' then have space and then they consist of a word\n",
    "# i describe this pattern as follows:\n",
    "re.findall(r'[dD]ata [A-Za-z]+',htmltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You can also use Regular expressions to modify a string\n",
    "def preprocesshtml(text):\n",
    "    # convert to ASCII\n",
    "\n",
    "    # if the input is HTML, force-add full stops after these tags\n",
    "    fullStopTags = ['li', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'dd']\n",
    "    for tag in fullStopTags:\n",
    "        text = re.sub(r'</'+tag+'>', '.', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)                  # strip out HTML\n",
    "    text = re.sub(r'[,:;()\\-]', ' ', text)               # replace commas, hyphens etc (count as spaces)\n",
    "    text = re.sub(r'[\\.!?]', '.', text)                  # unify terminators\n",
    "    text = re.sub(r'^\\s+', '', text)                     # strip leading whitespace\n",
    "    text = re.sub(r'[ ]*(\\n|\\r\\n|\\r)[ ]*', ' ', text)    # replace new lines with spaces\n",
    "    text = re.sub(r'([\\.])[\\. ]+', '.', text)            # check for duplicated terminators\n",
    "    text = re.sub(r'[ ]*([\\.])', '. ', text)             # pad sentence terminators\n",
    "    text = re.sub(r'\\s+', ' ', text)                     # remove multiple spaces\n",
    "    text = re.sub(r'\\s+$', '', text);                    # strip trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "htmltext ='''   <div class=\"container\">\n",
    "      <h3 class=\"quotes__subtitle chapeau-title chapeau-title--primary\">Testimonials</h3>\n",
    "      <h2 class=\"u-text-center\">Don’t just take our word for it.</h2>\n",
    "    <div class=\"quotes__wrapper js-quotes-height\">\n",
    "      <div class=\"js-quote\" data-index=\"0\" style=\"\">\n",
    "        <div class=\"quotes__quote\">The courses are excellent—they meet a higher standard than other platforms.</div>\n",
    "        <div class=\"quotes__description\">Helge is a PhD. Candidate in sociology and is learning data science tools to augment his research.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"1\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">Every month I learn something new on DataCamp, and they are all things that are very useful in my job.</div>\n",
    "        <div class=\"quotes__description\">Godefroy Clair is a Data Scientist at Flylab, where he helps create software for autonomous drones.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"2\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">DataCamp was essential for getting my job, which was my break into data science.</div>\n",
    "        <div class=\"quotes__description\">Jamen Long used the skills he learned at DataCamp to land a full time job as a data scientist.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"3\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">Data science is the future, and it is better to be on the cutting edge than left behind.</div>\n",
    "        <div class=\"quotes__description\">Arnaud uses techniques he&#39;s learned on DataCamp every day in his work for the French government.</div>\n",
    "      </div>\n",
    "      <div class=\"js-quote\" data-index=\"4\" style=\"display: none;\">\n",
    "        <div class=\"quotes__quote\">If anything, I regret I didn&#39;t start with DataCamp earlier.</div>\n",
    "        <div class=\"quotes__description\">Brigham Young University uses DataCamp to help students grasp data science concepts.</div>\n",
    "      </div>\n",
    "    </div>\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testimonials. Don’t just take our word for it. The courses are excellent—they meet a higher standard than other platforms. Helge is a PhD. Candidate in sociology and is learning data science tools to augment his research. Every month I learn something new on DataCamp and they are all things that are very useful in my job. Godefroy Clair is a Data Scientist at Flylab where he helps create software for autonomous drones. DataCamp was essential for getting my job which was my break into data science. Jamen Long used the skills he learned at DataCamp to land a full time job as a data scientist. Data science is the future and it is better to be on the cutting edge than left behind. Arnaud uses techniques he&#39 s learned on DataCamp every day in his work for the French government. If anything I regret I didn&#39 t start with DataCamp earlier. Brigham Young University uses DataCamp to help students grasp data science concepts.'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesshtml(htmltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testimonials Don’t just take our word for it The courses are excellentthey meet a higher standard than other platforms Helge is a PhD Candidate in sociology and is learning data science tools to augment his research Every month I learn something new on DataCamp and they are all things that are very useful in my job Godefroy Clair is a Data Scientist at Flylab where he helps create software for autonomous drones DataCamp was essential for getting my job which was my break into data science Jamen Long used the skills he learned at DataCamp to land a full time job as a data scientist Data science is the future and it is better to be on the cutting edge than left behind Arnaud uses techniques he39 s learned on DataCamp every day in his work for the French government If anything I regret I didn39 t start with DataCamp earlier Brigham Young University uses DataCamp to help students grasp data science concepts'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpunkt(preprocesshtml(htmltext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testimonials',\n",
       " 'don',\n",
       " '’',\n",
       " 'take',\n",
       " 'word',\n",
       " 'the',\n",
       " 'course',\n",
       " 'excellentthey',\n",
       " 'meet',\n",
       " 'higher',\n",
       " 'standard',\n",
       " 'platform',\n",
       " 'helge',\n",
       " 'phd',\n",
       " 'candidate',\n",
       " 'sociology',\n",
       " 'learning',\n",
       " 'data',\n",
       " 'science',\n",
       " 'tool',\n",
       " 'augment',\n",
       " 'research',\n",
       " 'every',\n",
       " 'month',\n",
       " 'i',\n",
       " 'learn',\n",
       " 'something',\n",
       " 'new',\n",
       " 'datacamp',\n",
       " 'thing',\n",
       " 'useful',\n",
       " 'job',\n",
       " 'godefroy',\n",
       " 'clair',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'flylab',\n",
       " 'help',\n",
       " 'create',\n",
       " 'software',\n",
       " 'autonomous',\n",
       " 'drone',\n",
       " 'datacamp',\n",
       " 'essential',\n",
       " 'getting',\n",
       " 'job',\n",
       " 'break',\n",
       " 'data',\n",
       " 'science',\n",
       " 'jamen',\n",
       " 'long',\n",
       " 'used',\n",
       " 'skill',\n",
       " 'learned',\n",
       " 'datacamp',\n",
       " 'land',\n",
       " 'full',\n",
       " 'time',\n",
       " 'job',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'data',\n",
       " 'science',\n",
       " 'future',\n",
       " 'better',\n",
       " 'cutting',\n",
       " 'edge',\n",
       " 'left',\n",
       " 'behind',\n",
       " 'arnaud',\n",
       " 'us',\n",
       " 'technique',\n",
       " 'he39',\n",
       " 'learned',\n",
       " 'datacamp',\n",
       " 'every',\n",
       " 'day',\n",
       " 'work',\n",
       " 'french',\n",
       " 'government',\n",
       " 'if',\n",
       " 'anything',\n",
       " 'i',\n",
       " 'regret',\n",
       " 'i',\n",
       " 'didn39',\n",
       " 'start',\n",
       " 'datacamp',\n",
       " 'earlier',\n",
       " 'brigham',\n",
       " 'young',\n",
       " 'university',\n",
       " 'us',\n",
       " 'datacamp',\n",
       " 'help',\n",
       " 'student',\n",
       " 'grasp',\n",
       " 'data',\n",
       " 'science',\n",
       " 'concept']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(unpunkt(preprocesshtml(htmltext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
