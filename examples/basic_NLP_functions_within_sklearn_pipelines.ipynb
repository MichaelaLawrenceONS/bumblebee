{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ```basicnlp``` functions to customise vectorizers within ```sklearn``` pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As José Blanco explained in his blogpost on towardsdatascience.com (https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af), one way to create a customised vectorizer is to instantiate the vectorizer using a modified version of Sklearn’s default analyzer.\n",
    "\n",
    "Under the hood, Sklearn’s vectorizers call a series of functions to convert a set of documents into a document-term matrix: \n",
    "- **```build_preprocessor```**: Returns a callable utilized to preprocess the input text before tokenization.\n",
    "- **```build_tokenizer:```** Creates a function capable of splitting a document’s corpus into tokens.\n",
    "- **```build_analyzer```**: Builds an analyzer function which applies preprocessing, tokenization, remove stop words and creates n-grams.\n",
    "\n",
    "By providing our own custom pipelines to either or all of these steps, we can customise the way we transform the documents for analysis. \n",
    "\n",
    "And this is exactly what we'll do in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ups and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and our user-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nlpfunctions.utils import *\n",
    "from nlpfunctions.basicnlp import *\n",
    "from nlpfunctions.nlppipelineutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For the purpose of this notebook, we will import the labelled text data used in 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015 (available here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "#print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_excel('Data/imdb.xlsx', header=0)\n",
    "yelp = pd.read_excel('Data/yelp_labelled.xlsx', header=0)\n",
    "\n",
    "imdb['source'] = 'imdb'\n",
    "yelp['source'] = 'yelp'\n",
    "\n",
    "df = pd.concat([imdb, yelp])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look a the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'score', 'source'], dtype='object')\n",
      "                                                text  score source\n",
      "0  A very, very, very slow-moving, aimless movie ...      0   imdb\n",
      "1  Not sure who was more lost - the flat characte...      0   imdb\n",
      "2  Attempting artiness with black & white and cle...      0   imdb\n",
      "3       Very little music or anything to speak of.        0   imdb\n",
      "4  The best scene in the movie was when Gerardo i...      1   imdb\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                 1742\n",
      "unique                1736\n",
      "top       I won't be back.\n",
      "freq                     2\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>imdb</th>\n",
       "      <td>361</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yelp</th>\n",
       "      <td>499</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "score     0    1\n",
       "source          \n",
       "imdb    361  384\n",
       "yelp    499  500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df['text'].describe())   #there are some duplicates\n",
    "\n",
    "pd.crosstab(df['source'], df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.duplicated('text')]\n",
    "df = df.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[pd.isnull(df['text'])]   #yep, 1 case\n",
    "df = df[pd.notnull(df['text'])]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include custom text-processing functions within sklearn ```CountVectorizer()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build an ad-hoc pre-processing pipeline\n",
    "\n",
    "For the pre-processing (i.e., before tokenisation), let's remove all sentences within each text that score too low on subjectivity score. We'll pick 0.3 as our subjectivity threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?remove_objective_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_preprocessor = combine_functions(sent_tokenise\n",
    "                                    , lambda x : remove_objective_sents(x, 0.3)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build ad-hoc word-tokenisation pipeline\n",
    "\n",
    "Let's then lemmatise, mark negations, remove numeric digits and punctuation as part of our tokenisation pipeline. \n",
    "Lemmatisaion requires that we first POS-tag the text.\n",
    "\n",
    "Importantly, the output of our pipeline must be a list of token lists as this is the required output for the next step in the vecorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tokenizer_pipe = combine_functions(word_tokenise\n",
    "                                       ,to_lower\n",
    "                                       ,POS_tagging\n",
    "                                       ,lemmatise\n",
    "                                       ,fix_neg_auxiliary\n",
    "                                       ,lambda x : remove_stopwords(x, extra_stopwords = [\n",
    "                                           'x', \"'s\", 'us', 'ca', 'many', 'much', 'one', 'put', '¬ñ',\n",
    "                                           'also', 'get', 'would', 'could', 'like', 'go', 'lot', 'make'\n",
    "                                       ])\n",
    "                                       ,lambda s: [[re.sub(r'\\d+','',x) for x in subs] for subs in s]\n",
    "                                       ,mark_neg\n",
    "                                       ,flattenIrregularListOfLists  # now we have one list of tokens per text/paragraph\n",
    "                                       ,remove_punctuation\n",
    "                                       ,lambda x: list(filter(None, x))   # must end with a list of token lists, each sublist is a paragraph/text\n",
    "                                       )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case you are wondering\n",
    "\n",
    "?fix_neg_auxiliary\n",
    "?mark_neg\n",
    "?flattenIrregularListOfLists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build ad-hoc ```CountVectorizer()```\n",
    "\n",
    "We can now use our ad-hoc preprocessor and tokenizer within ```CountVectorizer```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vec = CountVectorizer(preprocessor = my_preprocessor,\n",
    "                         tokenizer = my_tokenizer_pipe,\n",
    "                         analyzer=\"word\",\n",
    "                         ngram_range = (1,3),\n",
    "                         stop_words=None\n",
    "                         #min_df=1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instantiate our custom BoW pipeline\n",
    "\n",
    "We will use the ```Transformers``` ```ColumnSelector()``` and ```Series2ListOfStrings()``` to select the ```pandas.Series``` that contains the text data and transform it into a list of strings which is ```CountVecorizer()```'s required input format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_bags_words = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['text'])),\n",
    "        \n",
    "        ('transformer', Series2ListOfStrings()),\n",
    "        \n",
    "        ('vec', my_vec)\n",
    "        \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply our custom BoW pipeline to our data\n",
    "\n",
    "Let's take a look at the most frequent words first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>the</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>I</td>\n",
       "      <td>is</td>\n",
       "      <td>of</td>\n",
       "      <td>was</td>\n",
       "      <td>to</td>\n",
       "      <td>The</td>\n",
       "      <td>this</td>\n",
       "      <td>...</td>\n",
       "      <td>one</td>\n",
       "      <td>an</td>\n",
       "      <td>great</td>\n",
       "      <td>really</td>\n",
       "      <td>all</td>\n",
       "      <td>about</td>\n",
       "      <td>by</td>\n",
       "      <td>they</td>\n",
       "      <td>time</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1012</td>\n",
       "      <td>779</td>\n",
       "      <td>618</td>\n",
       "      <td>534</td>\n",
       "      <td>473</td>\n",
       "      <td>468</td>\n",
       "      <td>461</td>\n",
       "      <td>441</td>\n",
       "      <td>354</td>\n",
       "      <td>287</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8     9   ...    40  41  \\\n",
       "word    the  and    a    I   is   of  was   to  The  this  ...   one  an   \n",
       "count  1012  779  618  534  473  468  461  441  354   287  ...    74  71   \n",
       "\n",
       "          42      43   44     45  46    47    48    49  \n",
       "word   great  really  all  about  by  they  time  from  \n",
       "count     69      68   66     62  61    59    59    59  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "\n",
    "all_words = df['text'].str.split(expand=True).unstack().value_counts()\n",
    "all_words = all_words.to_frame().reset_index().rename(columns = {'index' : 'word', 0 : 'count'})\n",
    "\n",
    "# get 50 more frequent words, lots of \"rubbish\"\n",
    "#all_words[:50].plot.bar(x='word')\n",
    "all_words[:50].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0    A very, very, very slow-moving, aimless movie ...\n",
      "1    Not sure who was more lost - the flat characte...\n",
      "2    Attempting artiness with black & white and cle...\n",
      "3         Very little music or anything to speak of.  \n",
      "4    The best scene in the movie was when Gerardo i...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(type(df['text']))\n",
    "print(df['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average best</th>\n",
       "      <th>avoid</th>\n",
       "      <th>award</th>\n",
       "      <th>away</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awesome service</th>\n",
       "      <th>awful</th>\n",
       "      <th>baby</th>\n",
       "      <th>back</th>\n",
       "      <th>backNEG</th>\n",
       "      <th>backNEG NEG</th>\n",
       "      <th>bacon</th>\n",
       "      <th>bad</th>\n",
       "      <th>bad acting</th>\n",
       "      <th>bad bad</th>\n",
       "      <th>bad even</th>\n",
       "      <th>bad experience</th>\n",
       "      <th>bad film</th>\n",
       "      <th>bad film ever</th>\n",
       "      <th>bad food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average best  avoid  award  away  awesome  awesome service  awful  baby  \\\n",
       "1             0      0      0     0        0                0      0     0   \n",
       "2             0      0      0     0        0                0      0     0   \n",
       "3             0      0      0     0        0                0      0     0   \n",
       "4             0      0      0     0        0                0      0     0   \n",
       "5             0      0      0     0        0                0      0     0   \n",
       "\n",
       "   back  backNEG  backNEG NEG  bacon  bad  bad acting  bad bad  bad even  \\\n",
       "1     0        0            0      0    0           0        0         0   \n",
       "2     0        0            0      0    0           0        0         0   \n",
       "3     0        0            0      0    0           0        0         0   \n",
       "4     0        0            0      0    0           0        0         0   \n",
       "5     0        0            0      0    0           0        0         0   \n",
       "\n",
       "   bad experience  bad film  bad film ever  bad food  \n",
       "1               0         0              0         0  \n",
       "2               0         0              0         0  \n",
       "3               0         0              0         0  \n",
       "4               0         0              0         0  \n",
       "5               0         0              0         0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a random part of the output of our customised Vectoriser\n",
    "pd.DataFrame(pipe_bags_words.fit_transform(df).A, columns=my_vec.get_feature_names()).iloc[1:6, 100:120]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the customised Vectorizer to some new random data, using the vocabulary just learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average best</th>\n",
       "      <th>avoid</th>\n",
       "      <th>award</th>\n",
       "      <th>away</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awesome service</th>\n",
       "      <th>awful</th>\n",
       "      <th>baby</th>\n",
       "      <th>back</th>\n",
       "      <th>backNEG</th>\n",
       "      <th>backNEG NEG</th>\n",
       "      <th>bacon</th>\n",
       "      <th>bad</th>\n",
       "      <th>bad acting</th>\n",
       "      <th>bad bad</th>\n",
       "      <th>bad even</th>\n",
       "      <th>bad experience</th>\n",
       "      <th>bad film</th>\n",
       "      <th>bad film ever</th>\n",
       "      <th>bad food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average best  avoid  award  away  awesome  awesome service  awful  baby  \\\n",
       "0             0      0      0     0        0                0      0     0   \n",
       "\n",
       "   back  backNEG  backNEG NEG  bacon  bad  bad acting  bad bad  bad even  \\\n",
       "0     0        0            0      0    0           0        0         0   \n",
       "\n",
       "   bad experience  bad film  bad film ever  bad food  \n",
       "0               0         0              0         0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = pd.DataFrame([\"Would you tell me, please, which way I ought to go from here? That depends a good deal on where you want to get to. I don’t much care where! Then it doesn’t matter which way you go.\"], columns=['text'])\n",
    "\n",
    "pd.DataFrame(pipe_bags_words.transform(new_text).A, columns=my_vec.get_feature_names()).iloc[:, 100:120]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train an SVM classifier on the combined BoW features we have just computed with our customised vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bags_words = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['text'])),\n",
    "        \n",
    "        ('transformer', Series2ListOfStrings()),\n",
    "        \n",
    "        ('vec', my_vec),\n",
    "    \n",
    "        ('tf_idf', TfidfTransformer())\n",
    "        \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate pipeline\n",
    "\n",
    "pipe_bow_svm = Pipeline([\n",
    "        \n",
    "        ('bow', pipe_bags_words),\n",
    "        \n",
    "        ('classifier', svm)\n",
    "        \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check list of available hyperparameters\n",
    "\n",
    "pipe_bow_svm_cv.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters space and dictionary\n",
    "\n",
    "parameters = dict(\n",
    "        \n",
    "        #bow__vec__max_features = np.arange(5000, 11000, step=1000),\n",
    "    \n",
    "        classifier__kernel = ['linear', 'rbf'],\n",
    "        \n",
    "        classifier__C = [0.01, 0.1, 1, 10, 100]\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the GridSearchCV object: cv\n",
    "\n",
    "pipe_bow_svm_cv = GridSearchCV(estimator=pipe_bow_svm,\n",
    "                              param_grid=parameters,\n",
    "                              cv=5,\n",
    "                              scoring='accuracy'    #could be smth else, e.g., \"neg_log_loss\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe_bow_svm_cv.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a development dataset and an holdout dataset\n",
    "df_dev, df_holdout = train_test_split(df, test_size=0.20, stratify=df.score, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = df_dev[['text']]\n",
    "y_dev = df_dev.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter uning with GridSearch()\n",
    " \n",
    "pipe_bow_svm_cv.fit(X_dev, y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Tuned pipeline's Hyperparameters: {}\".format(pipe_bow_svm_cv.best_params_))\n",
    "print(\"Accuracy or best score: {}\".format(pipe_bow_svm_cv.best_score_))\n",
    "\n",
    "pipe_bow_svm_cv.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# Inspect results for each combination of parameters' value\n",
    "\n",
    "cv_results = pipe_bow_svm_cv.cv_results_\n",
    "cv_results.keys()\n",
    "\n",
    "for mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
    "    print(params, mean_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
