{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ```basicnlp``` and ```nlppipelineutils``` functions to include ad-hoc text and non-text features within ```sklearn``` pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example script, we build on the previous Example2 and expand the ```sklearn``` pipeline to include text and non-text ad-hoc features using functions and transformers from ```nlppipelineutils``` and ```sklearn FeatureUnion```.\n",
    "\n",
    "We will:\n",
    "1. Create a pipeline that calculates each text's Vader lexicon-based polarity score, and the proportion of adjectives and the count of meaningful punctuation symbols in each text as text ad-hoc features.\n",
    "2. Create a pipeline that includes the text's source as a non-text ad-hoc feature.\n",
    "3. Combine the BOW features and the text and non-text ad-hoc features into a unique pipeline using ```FeatureUnion```.\n",
    "4. Train and evaluate the pipeline with a SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ups and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and our user-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from nlpbumblebee.utils import *\n",
    "from nlpbumblebee.basicnlp import *\n",
    "from nlpbumblebee.nlppipelineutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the same labelled text data as in Example 2 from 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015 (available here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_excel('Data/imdb.xlsx', header=0)\n",
    "yelp = pd.read_excel('Data/yelp_labelled.xlsx', header=0)\n",
    "\n",
    "imdb['source'] = 'imdb'\n",
    "yelp['source'] = 'yelp'\n",
    "\n",
    "df = pd.concat([imdb, yelp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.duplicated('text')]\n",
    "df = df.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[pd.isnull(df['text'])]   #yep, 1 case\n",
    "df = df[pd.notnull(df['text'])]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and include ad-hoc text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric ad-hoc (text) features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a pipeline that extracts the following text-related information:\n",
    "- the proportion of adjectives \n",
    "- the count of meaningful punctuation symbols \n",
    "- ```NLTK``` Vader polarity score for each text (the average polarity scores off each text's senences)\n",
    "in each text and makes them available as text ad-hoc features for clasification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_punkt_fn = combine_functions(sent_tokenise\n",
    "                                       ,word_tokenise\n",
    "                                       ,lambda x: count_punkt(x, ['?', '!', '...'])\n",
    "                                       )   \n",
    "\n",
    "count_adj = combine_functions(sent_tokenise\n",
    "                                       ,word_tokenise\n",
    "                                       ,POS_tagging\n",
    "                                       ,lambda x: count_pos(x, pos_to_cnt='J', normalise=True)\n",
    "                                       )   \n",
    "\n",
    "VDR_score = combine_functions(sent_tokenise\n",
    "                                       ,get_sentiment_score_VDR\n",
    "                                       ,np.nanmean\n",
    "                                       )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pipeline:\n",
    "- We first select the dataset column that contains the text data;\n",
    "- We then use our custom ransformer ```Series2ListOfStrings``` to turn the ```pandas.Series``` into a list of strings on which we can apply our text-feature extracting functions. \n",
    "- We then extract the features by applying the ad-hoc text fetures extraction pipelines created above by using our custom ```TextPipelineArrayFeaturizer``` Transformer\n",
    "- Finally, as we are dealing with all numeric features, we scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to know more\n",
    "?ColumnSelector\n",
    "?Series2ListOfStrings\n",
    "?TextPipelineArrayFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?count_punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_text_features = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['text']))\n",
    "        \n",
    "        ,('transformer', Series2ListOfStrings())\n",
    "        \n",
    "        ,('text_featurizer', TextPipelineArrayFeaturizer(count_punkt_fn, count_adj, VDR_score))\n",
    "        \n",
    "        ,('scaler', StandardScaler())\n",
    "        \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the pipeline has done:\n",
    "- Each row in the array represents a text\n",
    "- the first column contains the proportion of adjectives in the text\n",
    "- the second columns contains the count of \"!\", \"?\" and \"...\" (our \"meaningfull\" punctuation) in the text\n",
    "- the third columns contain the text's VADER polarity score.\n",
    "\n",
    "Proportions, counts, and scores have been normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2810793 ,  1.80448558, -1.21046145],\n",
       "       [-0.2810793 ,  0.97468564, -0.4766906 ],\n",
       "       [-0.2810793 ,  1.15908563, -1.83865071],\n",
       "       ..., \n",
       "       [-0.2810793 ,  0.79028566, -1.10636394],\n",
       "       [-0.2810793 ,  0.60588567, -0.31683414],\n",
       "       [-0.2810793 , -1.05371421,  0.50470936]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on train data\n",
    "pipe_text_features.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the pipeline to some new random data. Mean and standard deviation of the scores learned for our original ```df.text``` data will be used to sandardise the scores of the new data using the ```transform``` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.09966208,  1.25128562, -0.37354714],\n",
       "       [ 0.84583449, -1.05371421, -0.42623193],\n",
       "       [ 0.84583449,  0.51368568, -0.31683414]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = pd.DataFrame(['I hate cheese!!! But it is soft and gentle and that makes it hard to accept.', \n",
    "                         'You should regret all opportunities you let go...',\n",
    "                         'There are several reasons. But I am going to tell her only one. Why? Because she deserves more.'], columns =['text'])\n",
    "\n",
    "pipe_text_features.transform(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and include ad-hoc non-text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ad-hoc binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to include features that are not computed from the texts themselevs but are, for instance, stored in other columns of the dataset?  \n",
    "\n",
    "For instance, we might want to include the source of the text, i.e., whether the text comes from imdb or yell, as we might have good reasons that people on yell tend to leave more positive feedback and so contribuing to predict the sentiment polarity of text. \n",
    "\n",
    "So here we create a pipeline that includes an existing ```pandas.DataFrame``` column containing some non-text ad-hoc features into our classification procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable ```df.source``` is of type string which makes our life easier. We can in fact use our custom Transfomer ```CatToDictTransformer``` to pass the ```pandas``` dataframe column directly to ```DictVectorizer``` as a list of dictionaries which ```DictVectorizer```'s expected input to obtain a numeric ('dummy') representation of the text's source (see more here http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat2dict = CatToDictTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CatToDictTransformer``` creates a list of dictionaries, with each dictionary containing the text as key and its value for source as value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'imdb'},\n",
       " {'source': 'imdb'},\n",
       " {'source': 'imdb'},\n",
       " {'source': 'imdb'},\n",
       " {'source': 'imdb'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2dict.fit_transform(df.source)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_bin_features = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['source'])),\n",
    "        \n",
    "        ('todictionary', CatToDictTransformer()),\n",
    "    \n",
    "        ('dv', DictVectorizer())\n",
    "                  \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the two numeric dummy variables represening whether the text's source is imdb or yell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  1.0  0.0\n",
       "1  1.0  0.0\n",
       "2  1.0  0.0\n",
       "3  1.0  0.0\n",
       "4  1.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pipe_bin_features.fit_transform(df).A).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc categorical multi-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our ad-hoc features have more than two levels? If the feature data is sored as string, we can still apply ```DictVectorizer``` to get the desired 'dummy' (or 'one-hot-encoding') representation. As it is shown here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this purpose, let's create a three-level categorical variables for our dataset. Let's randomly assign each text to one of three possible favourite music genres for their authors (jazz, pop, rock). Completely made up!  But, it is a truth universally acknowledged, that jazz lovers have better taste in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "genre = ['jazz', 'pop', 'rock']\n",
    "\n",
    "df['fav_music'] = random.choices(genre, k=df.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fav_music': 'pop'},\n",
       " {'fav_music': 'jazz'},\n",
       " {'fav_music': 'rock'},\n",
       " {'fav_music': 'pop'},\n",
       " {'fav_music': 'rock'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2dict.fit_transform(df.fav_music)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dv = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  0.0  1.0  0.0\n",
       "1  1.0  0.0  0.0\n",
       "2  0.0  0.0  1.0\n",
       "3  0.0  1.0  0.0\n",
       "4  0.0  0.0  1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dv.fit_transform(cat2dict.fit_transform(df.fav_music)[:5]).A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_cat_features = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['source', 'fav_music'])),\n",
    "        \n",
    "        ('todictionary', CatToDictTransformer()),\n",
    "    \n",
    "        ('dv', DictVectorizer())\n",
    "                  \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  0.0  1.0  0.0  1.0  0.0\n",
       "1  1.0  0.0  0.0  1.0  0.0\n",
       "2  0.0  0.0  1.0  1.0  0.0\n",
       "3  0.0  1.0  0.0  1.0  0.0\n",
       "4  0.0  0.0  1.0  1.0  0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pipe_cat_features.fit_transform(df).A).head(5)\n",
    "# note that the three dummy variables for fav_music are displayed first as columns 0, 1, 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use ```OneHotEncoder``` but ```OneHotEncoder``` does not work with string data which need to be conveted into integers first. One way to do so it's to use ```LabelEncoder``` firs. For the sake of completeness we will show it here.\n",
    "\n",
    "To achieve this, we also introduce another python module that helps bridge he gaps between ```pandas``` and ```sklearn```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE as of version 0.20 OneHotEncoder will deal with categorical sring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper(\n",
    "    [(d, LabelEncoder()) for d in ['source', 'fav_music']]\n",
    ")\n",
    "\n",
    "pipe_cat_features2 = Pipeline([\n",
    "    \n",
    "    (\"mapper\", mapper),\n",
    "    (\"ohe\", OneHotEncoder())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cat_features2.fit_transform(df).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: ```FeatureUnion``` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring it all together by creating a classification pipeline that combines:\n",
    "- bag-of-features (see Example 2)\n",
    "- ad-hoc text-related (numerical) features\n",
    "- ad-hoc non-text related (categorical) features\n",
    "\n",
    "We will use ```sklearn.pipeline.FeatureUnion``` to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-pipeline of ad-hoc (text and non-text) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ad-hoc features \n",
    "\n",
    "pipe_adhoc_features = Pipeline([\n",
    "        \n",
    "        ('adhoc', FeatureUnion([\n",
    "                \n",
    "                # pipeline for text-related (numerical) features\n",
    "                ('text_num', Pipeline([\n",
    "                    ('selector', ColumnSelector(columns=['text'])),\n",
    "                    ('transformer', Series2ListOfStrings()),\n",
    "                    ('text_featurizer', TextPipelineArrayFeaturizer(count_punkt_fn)),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ])),\n",
    "    \n",
    "                # pipeline for non-text (categorical) features\n",
    "                ('cat', Pipeline([\n",
    "                    ('selector', ColumnSelector(columns=['source', 'fav_music'])),\n",
    "                    ('todictionary', CatToDictTransformer()),\n",
    "                    ('dv', DictVectorizer())\n",
    "                ]))\n",
    "        ]))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-pipeline of BoW features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag-of-words\n",
    "\n",
    "my_preprocessor = combine_functions(sent_tokenise\n",
    "                                    , lambda x : remove_objective_sents(x, 0.3)\n",
    "                                    )\n",
    "\n",
    "my_tokenizer_pipe = combine_functions(word_tokenise\n",
    "                                       ,to_lower\n",
    "                                       ,POS_tagging\n",
    "                                       ,lemmatise\n",
    "                                       ,fix_neg_auxiliary\n",
    "                                       ,lambda x : remove_stopwords(x, extra_stopwords = [\n",
    "                                           'x', \"'s\", 'us', 'ca', 'many', 'much', 'one', 'put', '¬ñ',\n",
    "                                           'also', 'get', 'would', 'could', 'like', 'go', 'lot', 'make'\n",
    "                                       ])\n",
    "                                       ,lambda s: [[re.sub(r'\\d+','',x) for x in subs] for subs in s]\n",
    "                                       ,mark_neg\n",
    "                                       ,flattenIrregularListOfLists  # now we have one list of tokens per text\n",
    "                                       ,lambda x: list(filter(None, x))   # end with a list of token lists, each sublist is a text\n",
    "                                       )   \n",
    "\n",
    "my_vec = CountVectorizer(preprocessor = my_preprocessor,\n",
    "                         tokenizer = my_tokenizer_pipe,\n",
    "                         analyzer=\"word\",\n",
    "                         ngram_range = (1,3),\n",
    "                         stop_words=None\n",
    "                         #min_df=1\n",
    "                         )\n",
    "\n",
    "pipe_bags_words = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['text'])),\n",
    "        ('transformer', Series2ListOfStrings()),\n",
    "        ('vec', my_vec),\n",
    "        ('tf_idf', TfidfTransformer())\n",
    "        \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier\n",
    "\n",
    "svm = SVC(probability=True, C=1, kernel='linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_svm_clf = Pipeline([\n",
    "        \n",
    "        # Combined text (bag-of-word) and ad-hoc features\n",
    "        ('features', FeatureUnion(\n",
    "            \n",
    "                transformer_list = [\n",
    "                        ('adhoc', pipe_adhoc_features),\n",
    "                        ('text_bow', pipe_bags_words)\n",
    "                        ],\n",
    "            \n",
    "                # weight components in FeatureUnion\n",
    "                transformer_weights={\n",
    "                        'adhoc': 0.6,\n",
    "                        'text_bow': 1.0\n",
    "                        }    \n",
    "                )),\n",
    "        \n",
    "        # Use classifier on combined features\n",
    "        ('classifier', svm)\n",
    "        \n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation and hyperparameter tuning via GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's just run a cross-validation on the whole datase we have available \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['text', 'source', 'fav_music']]\n",
    "y = df.score.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_svm = cross_validate(pipe_svm_clf, X, y, cv = 5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.888 (std 0.003)\n",
      "Test Accuracy: 0.738 (std 0.059)\n"
     ]
    }
   ],
   "source": [
    "sorted(scores_svm.keys())\n",
    "\n",
    "print(\"Train Accuracy: %0.3f (std %0.3f)\" % (scores_svm['train_score'].mean(), scores_svm['train_score'].std()))\n",
    "print(\"Test Accuracy: %0.3f (std %0.3f)\" % (scores_svm['test_score'].mean(), scores_svm['test_score'].std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show that hyperparamter tuning works here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of available parameters\n",
    "\n",
    "#pipe_svm_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters space and dictionary\n",
    "\n",
    "parameters = dict(\n",
    "        \n",
    "        features__text_bow__vec__ngram_range = [(1,2), (1,3)],\n",
    "        \n",
    "        classifier__C = [0.1, 1],    \n",
    "        \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiate non_nested parameter search via GridSearch \n",
    "pipe_bow_svm_cv = GridSearchCV(estimator=pipe_svm_clf,\n",
    "                              param_grid=parameters,\n",
    "                              cv=cv,\n",
    "                              return_train_score=True,\n",
    "                              scoring='accuracy'    \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=123, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('adhoc', Pipeline(memory=None,\n",
       "     steps=[('adhoc', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text_num', Pipeline(memory=None,\n",
       "     steps=[('selector', ColumnSelector(columns=['text'])), ('transformer', Series2ListOfString...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'features__text_bow__vec__ngram_range': [(1, 2), (1, 3)], 'classifier__C': [0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow_svm_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'features__text_bow__vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bow_svm_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 0.1, 'features__text_bow__vec__ngram_range': (1, 2)} test score: 0.56 vs. train score: 0.604\n",
      "{'classifier__C': 0.1, 'features__text_bow__vec__ngram_range': (1, 3)} test score: 0.547 vs. train score: 0.591\n",
      "{'classifier__C': 1, 'features__text_bow__vec__ngram_range': (1, 2)} test score: 0.751 vs. train score: 0.887\n",
      "{'classifier__C': 1, 'features__text_bow__vec__ngram_range': (1, 3)} test score: 0.742 vs. train score: 0.889\n"
     ]
    }
   ],
   "source": [
    "dir(pipe_bow_svm_cv.cv_results_)\n",
    "\n",
    "for mean_test_score, mean_train_score, params in zip(pipe_bow_svm_cv.cv_results_['mean_test_score'], pipe_bow_svm_cv.cv_results_[\"mean_train_score\"], pipe_bow_svm_cv.cv_results_[\"params\"]):\n",
    "    print(params, 'test score: %s ' %round(mean_test_score, 3) + 'vs. train score: %s' %round(mean_train_score, 3))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
