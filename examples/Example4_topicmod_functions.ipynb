{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ```topicmod``` functions to easily apply topic modelling LDA and NMF techniques to text data stored in ```pandas.DataFrame``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example script, we show how ```nlpfunctions.topicmod``` functions can make it easy to apply Latent Derichlet Analysis (LDA) and Non-negative Matrix Factorization (NMF) models to text data and store the results in ```pandas.DataFrame``` so that they can be used in other analyses and explorations.\n",
    "\n",
    "We will:\n",
    "1. A\n",
    "2. B\n",
    "3. C\n",
    "4. D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ups and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and our user-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import bigfloat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from nltk import word_tokenize\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nlpfunctions.utils import *\n",
    "from nlpfunctions.basicnlp import *\n",
    "from nlpfunctions.nlppipelineutils import *\n",
    "from nlpfunctions.topicmod import *\n",
    "\n",
    "from gensim import models, corpora    #lda\n",
    "\n",
    "from sklearn import decomposition     #nmf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the same labelled text data as in Example 2 and 3 from \"From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\" (available here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_excel('Data/imdb.xlsx', header=0)\n",
    "df = pd.read_excel('Data/yelp_labelled.xlsx', header=0)\n",
    "\n",
    "#df['source'] = 'imdb'\n",
    "df['source'] = 'yelp'\n",
    "\n",
    "df[df.duplicated('text')]\n",
    "df = df.drop_duplicates('text')\n",
    "\n",
    "df[pd.isnull(df['text'])]   #yep, 1 case\n",
    "df = df[pd.notnull(df['text'])]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  score source\n",
      "0                           Wow... Loved this place.      1   yelp\n",
      "1                                 Crust is not good.      0   yelp\n",
      "2          Not tasty and the texture was just nasty.      0   yelp\n",
      "3  Stopped by during the late May bank holiday of...      1   yelp\n",
      "4  The selection on the menu was great and so wer...      1   yelp\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>the</td>\n",
       "      <td>and</td>\n",
       "      <td>I</td>\n",
       "      <td>was</td>\n",
       "      <td>a</td>\n",
       "      <td>to</td>\n",
       "      <td>The</td>\n",
       "      <td>is</td>\n",
       "      <td>of</td>\n",
       "      <td>for</td>\n",
       "      <td>...</td>\n",
       "      <td>place</td>\n",
       "      <td>with</td>\n",
       "      <td>had</td>\n",
       "      <td>be</td>\n",
       "      <td>are</td>\n",
       "      <td>were</td>\n",
       "      <td>very</td>\n",
       "      <td>that</td>\n",
       "      <td>have</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404</td>\n",
       "      <td>378</td>\n",
       "      <td>291</td>\n",
       "      <td>290</td>\n",
       "      <td>227</td>\n",
       "      <td>213</td>\n",
       "      <td>176</td>\n",
       "      <td>170</td>\n",
       "      <td>123</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...     15    16   17  \\\n",
       "word   the  and    I  was    a   to  The   is   of  for ...  place  with  had   \n",
       "count  404  378  291  290  227  213  176  170  123  102 ...     76    71   65   \n",
       "\n",
       "       18   19    20    21    22    23  24  \n",
       "word   be  are  were  very  that  have  so  \n",
       "count  64   62    61    60    59    59  58  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot most frequent words\n",
    "\n",
    "# https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "all_words = df['text'].str.split(expand=True).unstack().value_counts()\n",
    "all_words = all_words.to_frame().reset_index().rename(columns = {'index' : 'word', 0 : 'count'})\n",
    "\n",
    "# get 25 more frequent words\n",
    "all_words[:25].plot.bar(x='word')\n",
    "all_words[:25].T\n",
    "\n",
    "#lots of \"rubbish\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text data for topic modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will:\n",
    "1. tokenise\n",
    "2. lower case\n",
    "3. remove stopwords\n",
    "4. remove non-alphabetic tokens (i.e., punctuations and numbers)\n",
    "5. lemmatise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing_pipe = combine_functions(sent_tokenise\n",
    "                                       ,remove_punctuation\n",
    "                                       ,word_tokenise\n",
    "                                       ,to_lower\n",
    "                                       ,POS_tagging\n",
    "                                       ,lemmatise\n",
    "                                       ,fix_neg_auxiliary\n",
    "                                       ,lambda s: [[re.sub(r'\\d+','',x) for x in subs] for subs in s]\n",
    "                                       ,lambda x : remove_stopwords(x, extra_stopwords = [\n",
    "                                           'x', \"'s\", \"not\", 'us', 'no', 'many', 'much', 'one', 'put', 've',\n",
    "                                           'wo', 'even', 'first', 'may', 'late', 'come', 'iam', 'ive', 'ill',\n",
    "                                           'good', 'bad', 'great', 'sure', 'best', 'quite', 'per', 'due', 'always',\n",
    "                                           'say', 'want', 'm', 'ever', 'every', 'really', 'well', 'little', 'd',\n",
    "                                           'also', 'get', 'would', 'could', 'like', 'go', 'lot', 'make'])\n",
    "                                       ,flattenIrregularListOfLists\n",
    "                                       ,lambda x: list(filter(None, x))\n",
    "                                      )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>[wow, love, place]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>[crust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>[tasty, texture, nasty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>[stop, bank, holiday, rick, steve, recommendat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>[selection, menu, price]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                           Wow... Loved this place.   \n",
       "1                                 Crust is not good.   \n",
       "2          Not tasty and the texture was just nasty.   \n",
       "3  Stopped by during the late May bank holiday of...   \n",
       "4  The selection on the menu was great and so wer...   \n",
       "\n",
       "                                         text_lemmas  \n",
       "0                                 [wow, love, place]  \n",
       "1                                            [crust]  \n",
       "2                            [tasty, texture, nasty]  \n",
       "3  [stop, bank, holiday, rick, steve, recommendat...  \n",
       "4                           [selection, menu, price]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_lemmas'] = df['text'].apply(lambda x: preprocessing_pipe(x))\n",
    "\n",
    "# check some texts\n",
    "df[['text', 'text_lemmas']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>food</td>\n",
       "      <td>place</td>\n",
       "      <td>service</td>\n",
       "      <td>back</td>\n",
       "      <td>time</td>\n",
       "      <td>eat</td>\n",
       "      <td>love</td>\n",
       "      <td>wait</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>dont</td>\n",
       "      <td>...</td>\n",
       "      <td>think</td>\n",
       "      <td>experience</td>\n",
       "      <td>im</td>\n",
       "      <td>taste</td>\n",
       "      <td>pretty</td>\n",
       "      <td>staff</td>\n",
       "      <td>price</td>\n",
       "      <td>minute</td>\n",
       "      <td>pizza</td>\n",
       "      <td>wont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>123</td>\n",
       "      <td>110</td>\n",
       "      <td>84</td>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1        2     3     4    5     6     7           8     9   \\\n",
       "word   food  place  service  back  time  eat  love  wait  restaurant  dont   \n",
       "count   123    110       84    60    55   31    30    29          28    28   \n",
       "\n",
       "       ...      15          16  17     18      19     20     21      22  \\\n",
       "word   ...   think  experience  im  taste  pretty  staff  price  minute   \n",
       "count  ...      22          21  21     21      19     19     19      19   \n",
       "\n",
       "          23    24  \n",
       "word   pizza  wont  \n",
       "count     18    18  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-plot most frequent words\n",
    "\n",
    "all_lemmas = df['text_lemmas'].apply(list2string).str.split(expand=True).unstack().value_counts()\n",
    "all_lemmas = all_lemmas.to_frame().reset_index().rename(columns = {'index' : 'word', 0 : 'count'})\n",
    "\n",
    "# get 25 more frequent lemmas\n",
    "all_lemmas[:25].plot.bar(x='word')\n",
    "all_lemmas[:25].T\n",
    "\n",
    "# much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling with Latent Dirichlet Analysis (LDA)\n",
    "\n",
    "LDA is a topic discovery technique. It is a generative statistical topic model used to find accurate sets of topics within a given document set. The model assumes that text documents are comprised of a mixture of topics, and each topic is represented as the probability that each of given set of terms will occur. From there, using probability distributions the model can determine which topics are in a given document and which words are in a given topic based on word prevalence across topics and topic prevalence across document. A unique feature of LDA models is that topics are not required to be distinct, and words may occur in multiple topics\n",
    "\n",
    "Ref: \n",
    "\n",
    "https://medium.com/square-corner-blog/topic-modeling-optimizing-for-human-interpretability-48a81f6ce0ed\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note**: As of today, September 2018, there seems to be a bug introduced in a recent version of ```gensim```, the way it interacts with ```numpy``` which is used for all the computations. Downgrading to gensim 3.1.0 seems to solve the problem (ref: https://github.com/RaRe-Technologies/gensim/issues/2115).\n",
    "To do this please type ```$ pip install gensim==3.1.0``` in your Terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Create a dictionary containing the number of times a word appears in the corpus of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a Dictionary = association word to numeric id\n",
    "# assigning a unique integer id to each unique word while also collecting word counts and relevant statistics. \n",
    "dictionary = corpora.Dictionary(df['text_lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640\n"
     ]
    }
   ],
   "source": [
    "# what's the vocabulary size?\n",
    "print(len(dictionary.token2id.keys()))\n",
    "\n",
    "# take a look (first 25 entries in the dictionary)\n",
    "#for k, v in dictionary.token2id.items(): \n",
    "#    print(\"{} : {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Filter out words that occur too frequently or too rarely.\n",
    "\n",
    "When dealing with a bigger corpus than the one used in his example, you may want to further clean the text data by excluding words that occur in:\n",
    "- less than X texts (absolute number) or (infrequent words)\n",
    "- more than 0.p documents (fraction of total corpus size, not absolute number) (too frequent words).\n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_wordcount = 10\n",
    "max_freq = 0.6\n",
    "\n",
    "dictionary.filter_extremes(no_below=min_wordcount,\n",
    "                            no_above=max_freq,\n",
    "                            keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our dictionary size has become now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary.token2id.keys()))\n",
    "#...  too harsh filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) From texts as documents to Document Term Matrix\n",
    "\n",
    "Transform the collection of texts to a numerical form: For each text, report how many many times each occurring word appears. I.e., Convert the list of documents (corpus) into a Document Term Matrix using the dictionary prepared above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in df['text_lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "993\n",
      "[[(0, 1), (1, 1)], [], [(2, 1)], [(0, 1)], [(3, 1), (4, 1), (5, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(type(bow_corpus))\n",
    "print(len(bow_corpus))\n",
    "print(bow_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 text text_lemmas\n",
      "1  Crust is not good.     [crust]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at how the 1st text looks like: [(word_id, count), ...]\n",
    "\n",
    "print( df[['text', 'text_lemmas']][1:2] )\n",
    "print( bow_corpus[1] )\n",
    "\n",
    "for i in range(len(bow_corpus[1])):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus[1][i][0],\n",
    "          dictionary[bow_corpus[1][i][0]], \n",
    "          bow_corpus[1][i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Find the best number of topics\n",
    "\n",
    "#### Perplexity\n",
    "\n",
    "Perplexity is a standard measure for estimating the performance of a probabilistic model: it measures how well the probabilistic model predicts a sample. The perplexity of a set of test words, is defined as the exponential of the negative normalized predictive likelihood under the model. \n",
    "\n",
    "One should expect \"in-sample\" perplexity to improve with more topics, but that the improvement would level off as the model captures all but the most trivial structures in the data. So the ideal number of topics should be the poin where perplexity starts to level off. \n",
    "\n",
    "Ref: \n",
    "\n",
    "https://docs.google.com/viewer?a=v&pid=forums&srcid=MDEwMDM0NjcxOTk3Njc0MTA0MjMBMTQzMzY3Nzc1NTMzNDgyNjIyMzEBZnBOMFVLSG9BZ0FKATAuMwEBdjI&authuser=0\n",
    "\n",
    "https://groups.google.com/forum/#!topic/gensim/BDuOnCGpgOs\n",
    "\n",
    "http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
    "\n",
    "https://groups.google.com/forum/#!topic/gensim/TpuYRxhyIOc\n",
    "\n",
    "\n",
    "**Important**: However, please note that it has been shown that perplexity doesn't correlate well with human judgements of topic coherence. \n",
    "\n",
    "Other coherence measures have been suggested that have performed better. Example: https://www.kdnuggets.com/2016/07/americas-next-topic-model.html\n",
    "\n",
    "\n",
    "TODO: turn this into a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n",
      "305\n"
     ]
    }
   ],
   "source": [
    "# (i) divide corpus in training and test corpus. The test corpus will be used to calculate perplexity\n",
    "    \n",
    "shuffle(bow_corpus)\n",
    "\n",
    "train_corpus, test_corpus = bow_corpus[:800], bow_corpus[800:]\n",
    "\n",
    "# Number of words in the training set and in the test set\n",
    "print(sum(cnt for document in train_corpus for _, cnt in document))\n",
    "print(sum(cnt for document in test_corpus for _, cnt in document))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [(29, 1)]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics :  2\n",
      "per-word likelihood bound      -5.928503703089971\n",
      "perplexity : exp(-bound)                              375.59209586507086\n",
      "elapsed time: 12.765\n",
      " \n",
      "number of topics :  3\n",
      "per-word likelihood bound      -6.2177717511373665\n",
      "perplexity : exp(-bound)                              501.5843311729717\n",
      "elapsed time: 13.648\n",
      " \n",
      "number of topics :  4\n",
      "per-word likelihood bound      -6.317667816853502\n",
      "perplexity : exp(-bound)                              554.2788042103597\n",
      "elapsed time: 13.541\n",
      " \n",
      "number of topics :  5\n",
      "per-word likelihood bound      -6.4453385918376345\n",
      "perplexity : exp(-bound)                              629.759872419328\n",
      "elapsed time: 14.973\n",
      " \n",
      "number of topics :  7\n",
      "per-word likelihood bound      -6.435015227616049\n",
      "perplexity : exp(-bound)                              623.2920740437094\n",
      "elapsed time: 15.299\n",
      " \n",
      "number of topics :  8\n",
      "per-word likelihood bound      -6.478621206487419\n",
      "perplexity : exp(-bound)                              651.0726323970462\n",
      "elapsed time: 15.172\n",
      " \n",
      "number of topics :  9\n",
      "per-word likelihood bound      -6.407959961840749\n",
      "perplexity : exp(-bound)                              606.6548188480892\n",
      "elapsed time: 16.193\n",
      " \n",
      "number of topics :  10\n",
      "per-word likelihood bound      -6.354228115544517\n",
      "perplexity : exp(-bound)                              574.9183987703954\n",
      "elapsed time: 15.405\n",
      " \n",
      "number of topics :  15\n",
      "per-word likelihood bound      -6.347896168570844\n",
      "perplexity : exp(-bound)                              571.289546928367\n",
      "elapsed time: 16.475\n",
      " \n",
      "number of topics :  20\n",
      "per-word likelihood bound      -6.323077057929299\n",
      "perplexity : exp(-bound)                              557.2851555947809\n",
      "elapsed time: 17.590\n",
      " \n",
      "number of topics :  25\n",
      "per-word likelihood bound      -6.284294908197541\n",
      "perplexity : exp(-bound)                              536.0861673853265\n",
      "elapsed time: 18.239\n",
      " \n",
      "number of topics :  30\n",
      "per-word likelihood bound      -6.245844628815694\n",
      "perplexity : exp(-bound)                              515.8647552074955\n",
      "elapsed time: 19.629\n",
      " \n",
      "number of topics :  35\n",
      "per-word likelihood bound      -6.204676393533575\n",
      "perplexity : exp(-bound)                              495.0587258108987\n",
      "elapsed time: 21.742\n",
      " \n",
      "number of topics :  40\n",
      "per-word likelihood bound      -6.525852523782512\n",
      "perplexity : exp(-bound)                              682.5614255277253\n",
      "elapsed time: 22.211\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# (ii) loop on training set for several numbers of topics: \n",
    "topics_seq = list((2,3,4,5,7,8,9,10, 15, 20, 25, 30, 35))\n",
    "\n",
    "results_perplexity = {}\n",
    "for topic_n in topics_seq:\n",
    "    start_time = time.time()\n",
    "    # run model\n",
    "    print('number of topics :  %d' % topic_n)\n",
    "    \n",
    "    model = models.LdaModel(corpus=train_corpus\n",
    "                             , num_topics=topic_n\n",
    "                             , id2word=dictionary\n",
    "                             , passes = 20  # as we have a small corpus\n",
    "                             , eta = 0.01 # topics are known to be word-sparse, the Dirichlet parameter of the word distributions is set small (e.g., 0.01), in which case learning is efficient.\n",
    "                             , alpha = 0.1    #believed that each document is associated with few topics\n",
    "                             , random_state = 1\n",
    "                             )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # perplexity on hold-out test data\n",
    "    log_perplexity = model.log_perplexity(test_corpus)              # this is actually per-word likelihood bound\n",
    "    perplexity_test = np.exp(-log_perplexity.astype(np.float64))    # https://stats.stackexchange.com/a/324243\n",
    "    \n",
    "    print('per-word likelihood bound     ', log_perplexity)\n",
    "    print('perplexity : exp(-bound)                             ', perplexity_test)\n",
    "    print('elapsed time: %.3f' % elapsed)  \n",
    "    print( ' ')\n",
    "    results_perplexity[topic_n] = perplexity_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 375.59209586507086,\n",
       " 3: 501.5843311729717,\n",
       " 4: 554.2788042103597,\n",
       " 5: 629.759872419328,\n",
       " 7: 623.2920740437094,\n",
       " 8: 651.0726323970462,\n",
       " 9: 606.6548188480892,\n",
       " 10: 574.9183987703954,\n",
       " 15: 571.289546928367,\n",
       " 20: 557.2851555947809,\n",
       " 25: 536.0861673853265,\n",
       " 30: 515.8647552074955,\n",
       " 35: 495.0587258108987,\n",
       " 40: 682.5614255277253}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a30aa9828>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot of perplexity versus number of topics\n",
    "plt.plot(results_perplexity.keys(), results_perplexity.values(), 'r--',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpKREoSIiGT0/wE+sooAiMYrLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a307c7a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Perplexity')\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.grid(True) \n",
    "plt.show()\n",
    "\n",
    "# Perplexiy seems to suggest 8 topics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Build an lda model with the suggested number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the lda model with the suggested number of topics\n",
    "\n",
    "NUM_TOPICS = 8\n",
    "\n",
    "lda_model_1 = models.LdaModel(corpus=bow_corpus\n",
    "                             , num_topics=NUM_TOPICS\n",
    "                             , id2word=dictionary\n",
    "                             , passes = 20  # as we have a small corpus\n",
    "                             , eta = 0.01 # topics are known to be word-sparse, the Dirichlet parameter of the word distributions is set small (e.g., 0.01), in which case learning is efficient.\n",
    "                             , alpha = 0.1    #believed that each document is associated with few topics\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Explore topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the topics, we will look at each topic in terms of the words it has the highest probability to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic #0: 0.176*\"wait\" + 0.128*\"experience\" + 0.096*\"try\" + 0.086*\"fry\" + 0.073*\"pizza\"\n",
      "Topic #1: 0.406*\"service\" + 0.135*\"food\" + 0.068*\"meal\" + 0.063*\"quality\" + 0.058*\"fantastic\"\n",
      "Topic #2: 0.252*\"place\" + 0.124*\"restaurant\" + 0.115*\"friendly\" + 0.096*\"love\" + 0.084*\"staff\"\n",
      "Topic #3: 0.279*\"time\" + 0.130*\"delicious\" + 0.091*\"take\" + 0.088*\"chicken\" + 0.068*\"steak\"\n",
      "Topic #4: 0.150*\"eat\" + 0.109*\"dont\" + 0.104*\"pretty\" + 0.094*\"place\" + 0.084*\"nice\"\n",
      "Topic #5: 0.092*\"order\" + 0.088*\"taste\" + 0.075*\"star\" + 0.071*\"dish\" + 0.067*\"salad\"\n",
      "Topic #6: 0.359*\"food\" + 0.094*\"think\" + 0.068*\"never\" + 0.060*\"flavor\" + 0.059*\"fresh\"\n",
      "Topic #7: 0.319*\"back\" + 0.096*\"wont\" + 0.087*\"definitely\" + 0.073*\"way\" + 0.064*\"awesome\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# (iv) Explore Topics\n",
    "\n",
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 5 most representative words for each topic\n",
    "    print(\"Topic #%s:\" % idx, lda_model_1.print_topic(idx, 5))\n",
    " \n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Really hard to understand the topics...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python package ```pyLDAvis``` is designed to help the interpretion of the topics in a topic model. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization (https://datascienceplus.com/topic-modeling-in-python-with-nltk-and-gensim/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el9267112477245016521021410\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el9267112477245016521021410_data = {\"mdsDat\": {\"x\": [0.29311866831483735, -0.2872943486240382, 0.12713891895435836, -0.011612654163065547, -0.08186498824122455, 0.17757964465935422, -0.24994541716461677, 0.03288017626439494], \"y\": [-0.05521458958635283, -0.036644097832132724, 0.1960355636738221, -0.014259053879849846, 0.1713133645160283, 0.09417009249448663, 0.03693095275994485, -0.392332232145947], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.815223844996233, 12.68491274583743, 13.356824022933111, 11.574316618459484, 11.767610452864478, 13.990988363339529, 13.968818829792056, 11.841305121777687]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"Freq\": [83.0, 60.0, 119.0, 57.0, 107.0, 30.0, 32.0, 26.0, 24.0, 24.0, 22.0, 28.0, 19.0, 21.0, 18.0, 19.0, 25.0, 29.0, 18.0, 16.0, 17.0, 18.0, 14.0, 17.0, 24.0, 18.0, 16.0, 20.0, 15.0, 15.0, 22.357058406518618, 14.913973904852293, 11.720714334981217, 30.668021152122495, 12.691511232927322, 16.68030694655494, 8.52136225344228, 7.463964425707572, 12.778239718971722, 9.5864404784639, 7.557629050403303, 4.183562071128147, 2.4758198279862906, 6.825869378788155, 3.4061918023282423, 1.4109392804125471, 0.0718433749060923, 0.032184593669678484, 0.017347806734511306, 0.048378058590870424, 0.01820850010852122, 0.013051400434215753, 0.012208078928380956, 0.02140806740776905, 0.012319473591209627, 0.02022984322474752, 0.010648597050668765, 0.011183451717647735, 0.010648608274641504, 0.010648593857306262, 0.024389588284929396, 0.014444765762447893, 0.011451032148632013, 0.014748367620922713, 0.024339498836751856, 0.01302654705425734, 0.012573648309396555, 82.95494547590266, 13.873128784283363, 12.876160989344642, 11.850143300656464, 10.866276079714739, 10.85473286611863, 9.21115524821281, 4.19027406712291, 5.94588456128799, 2.9818897835355918, 5.056768772888852, 27.57386922371393, 4.553280244221028, 0.48072296393840946, 0.15006884155966735, 0.07086265763313099, 0.033866595818941296, 0.027597245068667564, 0.01386469165578112, 0.01348160535164291, 0.02274335511007664, 0.013370616958733908, 0.009906778325848081, 0.009906834477785861, 0.010474012643994201, 0.009906818250688535, 0.009906819496602826, 0.009906794069085397, 0.0099068011878619, 0.010404290989849945, 0.011961087343825656, 0.01654464681154783, 0.010739266757607308, 0.014752963387827242, 0.014136683571162075, 0.013809397091902901, 0.012118174298299778, 0.013139863179416218, 0.01700865403104008, 24.830997690932545, 26.71533214178624, 18.135515209764662, 11.457436958025362, 9.013036371441274, 9.251198907541255, 20.719878602433006, 12.093345814576402, 5.744919027597743, 7.501671108293833, 54.112753680931306, 6.954512522124024, 2.9293740652644935, 1.9238554410167161, 2.526978743425402, 0.0570273005483729, 0.054935058379242833, 0.05813004302530489, 0.2687266724641136, 0.028761938702362974, 0.025584076802981793, 0.011817937425028267, 0.012297731900589212, 0.010683382609189088, 0.009548715768930774, 0.010160740810704472, 0.013346948836155878, 0.009548710509001404, 0.010267189652736963, 0.009548731960933005, 0.011371854692532027, 0.011546949642357283, 0.016078676123929322, 0.050724217161470284, 0.013221917907302295, 0.013209549513088443, 0.014268865252221223, 24.2604916213495, 16.879747349695304, 16.348865447124737, 52.00247055695606, 7.779197346614305, 12.672742560331564, 6.979275384168572, 12.583891588902157, 3.6348788815899074, 4.276492534737546, 3.6344882182530402, 11.07758376725697, 1.449141208296355, 1.0660882014952957, 2.0116068374971707, 7.861379915792286, 0.3654844688476369, 0.27886595202498515, 0.08367673426343887, 0.20867218641540258, 0.05799910728712615, 0.05315805169856253, 0.04177951214593883, 0.028662326799239433, 0.030909209789698966, 0.021400195447509363, 0.03518917561189508, 0.01846034097605629, 0.014442944477773239, 0.015281507600448335, 0.01825267724508491, 0.023235180264471217, 19.62881705091364, 13.439523582341007, 13.438266750366367, 12.405648611273016, 28.453688465648447, 20.62859651788458, 13.338631750098417, 8.382301666959258, 15.878976759003608, 3.5002079779969684, 4.142526415287961, 3.111528409241314, 4.138457612976951, 2.076619860605035, 2.074569823495934, 17.721254693201075, 1.0434190650351436, 1.2488616525228622, 3.6823512872612523, 0.07301653264951591, 0.31937907393124776, 0.030031513628083196, 0.035602522558834226, 0.018216383387504857, 0.012078039257824556, 0.014536347629493905, 0.01033149324810804, 0.01033150415372599, 0.010850923732352337, 0.012672199159436612, 0.01839555829891834, 0.02842822428398232, 0.012539429055218343, 0.01632427735217339, 0.014536367924940617, 0.01622229129525997, 0.016521577472672244, 19.77326232891757, 16.012097137576568, 16.94775373767347, 15.067665854820909, 12.243774080213585, 15.047343031526898, 20.67189698777113, 12.049100236957612, 14.882365988598757, 11.136529882170828, 7.538735938514026, 6.535499564404584, 7.491983878092836, 5.445105846104502, 8.221516362600372, 4.715894617191044, 5.648011622815291, 2.412716120271996, 4.168231250170197, 2.015133975245064, 1.8443037065679089, 1.2095207064584457, 12.00001831985448, 1.2758022611268072, 0.20398781005930441, 0.030334040952864426, 0.10618404698688176, 0.013453234251431133, 0.02710461064438756, 0.009887845400329958, 0.017171204461803886, 0.020622763940754615, 21.10686436012355, 11.522481454039298, 10.560870906979044, 13.418522881343126, 12.461706237486792, 13.304169300880128, 12.027415245118595, 80.77707882514717, 15.27744207092501, 6.083670308390885, 7.683074218766844, 2.689181226092742, 1.2196573219288265, 12.453853062492586, 1.2766835883254928, 1.5762168946676263, 0.5543185954902564, 0.0640501725690974, 0.0323849349567903, 0.11921751209695408, 0.040032595102055146, 0.020646909123266454, 0.01151847989325073, 0.016705393961369806, 0.011701837015005621, 0.010561400091202398, 0.00959745306377388, 0.009597436839954884, 0.010080787881482695, 0.009597446563837824, 0.011801859105027944, 0.04538741397946271, 0.011463281240637007, 0.014120594493635941, 0.012367513364223525, 0.012209203435810154, 0.011966063828238952, 60.817520806416795, 18.26537731198496, 12.178096976313428, 11.095962629290069, 10.056958605494062, 13.857031478846043, 16.596696519080083, 8.106678240009291, 10.803221551415945, 9.218410853944132, 3.3672304441530914, 5.279081323703985, 2.554777783489368, 1.7100472302840306, 5.141064707681882, 0.7017289996155762, 0.031278312343718404, 0.02535792182834172, 0.027895491464191118, 0.020148372753288398, 0.12186214656920268, 0.011261155394799797, 0.010143947612817441, 0.010143916563019181, 0.010653281840206485, 0.010653270288757566, 0.010143931378167664, 0.010143943882217158, 0.017567020043605636, 0.011997495013620628, 0.020174922031548602, 0.015805211629508025, 0.011926221072374753, 0.01614606653906069, 0.03241291734536233, 0.01167227549654858, 0.013964572822364057, 0.012719990276035318, 0.013473702752591707, 0.012284233123939923, 0.012287553172872069], \"Term\": [\"service\", \"back\", \"food\", \"time\", \"place\", \"wait\", \"eat\", \"restaurant\", \"delicious\", \"friendly\", \"experience\", \"dont\", \"pretty\", \"think\", \"wont\", \"taste\", \"order\", \"love\", \"staff\", \"take\", \"try\", \"definitely\", \"fry\", \"star\", \"never\", \"chicken\", \"dish\", \"minute\", \"salad\", \"disappointed\", \"experience\", \"fry\", \"cant\", \"wait\", \"selection\", \"try\", \"leave\", \"perfect\", \"pizza\", \"table\", \"minute\", \"burger\", \"amazing\", \"love\", \"give\", \"chicken\", \"find\", \"thing\", \"cook\", \"dont\", \"recommend\", \"worth\", \"everything\", \"steak\", \"next\", \"definitely\", \"side\", \"meat\", \"excellent\", \"serve\", \"never\", \"star\", \"enough\", \"taste\", \"back\", \"delicious\", \"time\", \"service\", \"meal\", \"quality\", \"fantastic\", \"tasty\", \"atmosphere\", \"side\", \"cook\", \"steak\", \"perfect\", \"server\", \"food\", \"nice\", \"sushi\", \"fresh\", \"everything\", \"thing\", \"give\", \"enjoy\", \"leave\", \"pizza\", \"wasnt\", \"worth\", \"amazing\", \"slow\", \"excellent\", \"serve\", \"find\", \"meat\", \"night\", \"flavor\", \"think\", \"awesome\", \"order\", \"dont\", \"wait\", \"pretty\", \"restaurant\", \"place\", \"friendly\", \"restaurant\", \"staff\", \"night\", \"worth\", \"recommend\", \"love\", \"price\", \"amazing\", \"feel\", \"place\", \"menu\", \"table\", \"everything\", \"nice\", \"excellent\", \"find\", \"atmosphere\", \"time\", \"fantastic\", \"thing\", \"meat\", \"bland\", \"leave\", \"cook\", \"perfect\", \"meal\", \"side\", \"next\", \"serve\", \"amaze\", \"wasnt\", \"definitely\", \"service\", \"server\", \"pizza\", \"dont\", \"delicious\", \"take\", \"chicken\", \"time\", \"find\", \"steak\", \"meat\", \"minute\", \"next\", \"order\", \"eat\", \"place\", \"vega\", \"thing\", \"love\", \"food\", \"try\", \"burger\", \"still\", \"wait\", \"excellent\", \"tasty\", \"selection\", \"serve\", \"wasnt\", \"side\", \"restaurant\", \"disappointed\", \"fantastic\", \"quality\", \"think\", \"service\", \"pretty\", \"didnt\", \"know\", \"buffet\", \"eat\", \"dont\", \"server\", \"bland\", \"nice\", \"meat\", \"feel\", \"slow\", \"vega\", \"serve\", \"leave\", \"place\", \"everything\", \"thing\", \"food\", \"selection\", \"time\", \"side\", \"flavor\", \"quality\", \"amazing\", \"fresh\", \"worth\", \"cook\", \"excellent\", \"fantastic\", \"price\", \"wait\", \"recommend\", \"im\", \"take\", \"delicious\", \"service\", \"taste\", \"dish\", \"star\", \"salad\", \"amaze\", \"disappointed\", \"order\", \"give\", \"im\", \"burger\", \"slow\", \"everything\", \"thing\", \"cook\", \"menu\", \"next\", \"pizza\", \"bland\", \"nice\", \"feel\", \"table\", \"find\", \"place\", \"way\", \"chicken\", \"fantastic\", \"service\", \"leave\", \"delicious\", \"amazing\", \"staff\", \"love\", \"think\", \"enough\", \"enjoy\", \"flavor\", \"wasnt\", \"fresh\", \"sushi\", \"food\", \"never\", \"price\", \"dont\", \"thing\", \"find\", \"place\", \"bland\", \"definitely\", \"worth\", \"next\", \"feel\", \"time\", \"eat\", \"take\", \"meat\", \"burger\", \"atmosphere\", \"side\", \"everything\", \"cook\", \"serve\", \"amazing\", \"recommend\", \"back\", \"amaze\", \"staff\", \"salad\", \"steak\", \"experience\", \"back\", \"wont\", \"awesome\", \"still\", \"excellent\", \"way\", \"definitely\", \"serve\", \"vega\", \"never\", \"recommend\", \"im\", \"next\", \"amazing\", \"time\", \"side\", \"bland\", \"meat\", \"disappointed\", \"night\", \"service\", \"worth\", \"everything\", \"cook\", \"enjoy\", \"leave\", \"find\", \"perfect\", \"price\", \"buffet\", \"experience\", \"try\", \"sushi\", \"pretty\", \"food\", \"flavor\", \"order\", \"staff\", \"eat\", \"take\", \"restaurant\"], \"Total\": [83.0, 60.0, 119.0, 57.0, 107.0, 30.0, 32.0, 26.0, 24.0, 24.0, 22.0, 28.0, 19.0, 21.0, 18.0, 19.0, 25.0, 29.0, 18.0, 16.0, 17.0, 18.0, 14.0, 17.0, 24.0, 18.0, 16.0, 20.0, 15.0, 15.0, 22.44109830561972, 14.987682306663254, 11.793286519911529, 30.958192291602497, 12.856035490684254, 17.113916319203778, 10.66435706502452, 10.506058942443293, 18.507074916766516, 14.411264964342386, 20.201956683538885, 15.656641836282033, 9.983342383304052, 29.62095869025678, 15.538804807193898, 18.013803102896347, 10.365536020378057, 12.59840360317187, 9.703435427405616, 28.421705077157306, 12.691329926918797, 9.631876804446065, 9.616808718940339, 18.69168366965665, 11.012426435271507, 18.24996373036442, 10.004489660412624, 10.55915573575514, 10.222876546488408, 10.262158910666614, 24.571057903755204, 17.027984352956206, 11.597344865749697, 19.852367636010648, 60.94183970082438, 24.356528568857705, 57.8842535192619, 83.29371988720179, 13.949345629752246, 12.959547450481734, 11.966744556594918, 10.98011759184614, 10.975659941698265, 10.004489660412624, 9.703435427405616, 18.69168366965665, 10.506058942443293, 18.45998389958786, 119.96347508469074, 27.169017964007455, 12.571555988422253, 13.521262051698155, 9.616808718940339, 12.59840360317187, 15.538804807193898, 10.637545508687205, 10.66435706502452, 18.507074916766516, 12.55872249839434, 9.631876804446065, 9.983342383304052, 10.711763168327318, 10.222876546488408, 10.262158910666614, 10.365536020378057, 10.55915573575514, 11.539135917808869, 13.51840006316064, 21.195447572115267, 12.252696290129927, 25.0207740014927, 28.421705077157306, 30.958192291602497, 19.70945661819358, 26.81909196502287, 107.40667037012673, 24.906941221151452, 26.81909196502287, 18.22279972695084, 11.539135917808869, 9.631876804446065, 12.691329926918797, 29.62095869025678, 18.25445117866546, 9.983342383304052, 13.732971979830612, 107.40667037012673, 15.241664389097233, 14.411264964342386, 9.616808718940339, 27.169017964007455, 10.222876546488408, 10.365536020378057, 10.975659941698265, 57.8842535192619, 11.966744556594918, 12.59840360317187, 10.55915573575514, 12.146388839473193, 10.66435706502452, 9.703435427405616, 10.506058942443293, 13.949345629752246, 10.004489660412624, 11.012426435271507, 10.262158910666614, 12.319784807608205, 12.55872249839434, 18.24996373036442, 83.29371988720179, 18.45998389958786, 18.507074916766516, 28.421705077157306, 24.356528568857705, 16.96831877092199, 18.013803102896347, 57.8842535192619, 10.365536020378057, 18.69168366965665, 10.55915573575514, 20.201956683538885, 11.012426435271507, 25.0207740014927, 32.18222565145643, 107.40667037012673, 16.44136839974385, 12.59840360317187, 29.62095869025678, 119.96347508469074, 17.113916319203778, 15.656641836282033, 11.241221868622329, 30.958192291602497, 10.222876546488408, 10.98011759184614, 12.856035490684254, 10.262158910666614, 12.55872249839434, 10.004489660412624, 26.81909196502287, 15.146343985939625, 11.966744556594918, 12.959547450481734, 21.195447572115267, 83.29371988720179, 19.70945661819358, 13.510948061680338, 13.510836706700031, 12.47779868638536, 32.18222565145643, 28.421705077157306, 18.45998389958786, 12.146388839473193, 27.169017964007455, 10.55915573575514, 13.732971979830612, 10.711763168327318, 16.44136839974385, 10.262158910666614, 10.66435706502452, 107.40667037012673, 9.616808718940339, 12.59840360317187, 119.96347508469074, 12.856035490684254, 57.8842535192619, 10.004489660412624, 13.51840006316064, 12.959547450481734, 9.983342383304052, 13.521262051698155, 9.631876804446065, 9.703435427405616, 10.222876546488408, 11.966744556594918, 18.25445117866546, 30.958192291602497, 12.691329926918797, 20.230905616634743, 16.96831877092199, 24.356528568857705, 83.29371988720179, 19.852367636010648, 16.085732250817728, 17.027984352956206, 15.144318088434169, 12.319784807608205, 15.146343985939625, 25.0207740014927, 15.538804807193898, 20.230905616634743, 15.656641836282033, 10.711763168327318, 9.616808718940339, 12.59840360317187, 9.703435427405616, 15.241664389097233, 11.012426435271507, 18.507074916766516, 12.146388839473193, 27.169017964007455, 13.732971979830612, 14.411264964342386, 10.365536020378057, 107.40667037012673, 15.198035512142663, 18.013803102896347, 11.966744556594918, 83.29371988720179, 10.66435706502452, 24.356528568857705, 9.983342383304052, 18.22279972695084, 29.62095869025678, 21.195447572115267, 11.597344865749697, 10.637545508687205, 13.51840006316064, 12.55872249839434, 13.521262051698155, 12.571555988422253, 119.96347508469074, 24.571057903755204, 18.25445117866546, 28.421705077157306, 12.59840360317187, 10.365536020378057, 107.40667037012673, 12.146388839473193, 18.24996373036442, 9.631876804446065, 11.012426435271507, 13.732971979830612, 57.8842535192619, 32.18222565145643, 16.96831877092199, 10.55915573575514, 15.656641836282033, 10.975659941698265, 10.004489660412624, 9.616808718940339, 9.703435427405616, 10.262158910666614, 9.983342383304052, 12.691329926918797, 60.94183970082438, 12.319784807608205, 18.22279972695084, 15.144318088434169, 18.69168366965665, 22.44109830561972, 60.94183970082438, 18.33910879811496, 12.252696290129927, 11.241221868622329, 10.222876546488408, 15.198035512142663, 18.24996373036442, 10.262158910666614, 16.44136839974385, 24.571057903755204, 12.691329926918797, 20.230905616634743, 11.012426435271507, 9.983342383304052, 57.8842535192619, 10.004489660412624, 12.146388839473193, 10.55915573575514, 15.146343985939625, 11.539135917808869, 83.29371988720179, 9.631876804446065, 9.616808718940339, 9.703435427405616, 10.637545508687205, 10.66435706502452, 10.365536020378057, 10.506058942443293, 18.25445117866546, 12.47779868638536, 22.44109830561972, 17.113916319203778, 12.571555988422253, 19.70945661819358, 119.96347508469074, 13.51840006316064, 25.0207740014927, 18.22279972695084, 32.18222565145643, 16.96831877092199, 26.81909196502287], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2205, 2.2193, 2.218, 2.2148, 2.2113, 2.1986, 1.9999, 1.8823, 1.8538, 1.8166, 1.241, 0.9045, 0.8299, 0.7565, 0.7065, -0.3227, -2.7475, -3.7456, -4.1026, -4.1516, -4.3226, -4.3797, -4.445, -4.5479, -4.5714, -4.5805, -4.6211, -4.6261, -4.6427, -4.6466, -4.691, -4.8481, -4.6962, -4.9807, -5.6014, -5.3094, -6.2104, 2.0607, 2.0593, 2.0583, 2.055, 2.0543, 2.0537, 1.9821, 1.225, 0.9194, 0.8054, 0.7699, 0.5944, 0.2785, -1.1991, -2.4362, -2.8458, -3.8541, -4.2686, -4.578, -4.6086, -4.6369, -4.7804, -4.8149, -4.8507, -4.8654, -4.8744, -4.8782, -4.8883, -4.9068, -4.9465, -4.9654, -5.0907, -4.9748, -5.3713, -5.5414, -5.6503, -5.3294, -5.5565, -6.6859, 2.0101, 2.0093, 2.0083, 2.006, 1.9467, 1.697, 1.6558, 1.6014, 1.4605, 1.4085, 1.3276, 1.2285, 0.4199, 0.404, -0.3619, -3.1757, -3.2269, -3.2276, -3.3594, -4.0177, -4.1862, -4.782, -4.8822, -4.8928, -4.9107, -4.928, -4.9388, -4.9412, -4.9647, -4.9667, -4.9747, -4.9786, -5.0213, -5.3906, -5.2283, -5.2318, -5.5837, 2.1524, 2.1511, 2.0594, 2.0492, 1.8693, 1.7678, 1.7423, 1.683, 1.0479, 0.3898, -0.0246, -0.1153, -0.2724, -0.3132, -0.5332, -0.5688, -1.69, -1.8715, -2.744, -2.8432, -3.0156, -3.1742, -3.5728, -3.7243, -3.8507, -3.991, -4.4797, -4.5535, -4.5633, -4.5866, -4.9008, -6.0281, 2.1357, 2.1345, 2.1344, 2.134, 2.0167, 1.8193, 1.8149, 1.7689, 1.6027, 1.0356, 0.9413, 0.9036, 0.7603, 0.5421, 0.5027, 0.338, -0.0812, -0.1715, -1.3438, -3.0311, -3.06, -3.6687, -3.7996, -4.4274, -4.5775, -4.6955, -4.6978, -4.7052, -4.7083, -4.7107, -4.7602, -4.8532, -4.78, -4.9825, -4.9226, -5.1743, -6.3856, 1.9628, 1.9622, 1.962, 1.9617, 1.9606, 1.9602, 1.7758, 1.7124, 1.6597, 1.6261, 1.6155, 1.5805, 1.447, 1.389, 1.3495, 1.1187, 0.7799, 0.3505, 0.0922, 0.0476, -0.0892, -0.1815, -0.225, -0.5108, -2.5141, -4.0109, -4.6982, -4.7087, -4.8341, -4.9506, -5.0004, -5.3031, 1.9642, 1.9619, 1.9611, 1.9609, 1.9606, 1.9522, 1.9241, 1.5728, 1.4932, 0.8695, 0.6602, 0.424, -0.1716, -0.1862, -0.2844, -0.4808, -0.8868, -3.1788, -4.0815, -4.2169, -4.7211, -4.7432, -4.8525, -4.8746, -4.8753, -4.8852, -4.9414, -4.9504, -4.9572, -4.9788, -5.0121, -5.2341, -5.0115, -5.1945, -5.142, -5.3653, -5.5682, 2.1315, 2.1295, 2.1275, 2.1206, 2.1172, 2.0412, 2.0386, 1.8978, 1.7136, 1.1532, 0.8067, 0.7901, 0.6725, 0.3692, -0.2876, -0.5237, -3.8283, -3.8981, -4.1635, -4.2168, -4.3937, -4.6179, -4.7208, -4.7298, -4.7727, -4.7752, -4.7958, -4.8093, -4.8126, -4.8134, -4.8806, -4.8537, -4.8269, -4.9736, -6.0828, -4.921, -5.3574, -5.1337, -5.6449, -5.0972, -5.5547], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.0526, -2.4575, -2.6984, -1.7366, -2.6188, -2.3455, -3.0172, -3.1497, -2.612, -2.8994, -3.1372, -3.7286, -4.2532, -3.2391, -3.9342, -4.8155, -7.793, -8.596, -9.2141, -8.1885, -9.1656, -9.4986, -9.5654, -9.0038, -9.5563, -9.0604, -9.7021, -9.6531, -9.7021, -9.7021, -8.8734, -9.3972, -9.6294, -9.3764, -8.8754, -9.5005, -9.5359, -0.9009, -2.6893, -2.7639, -2.8469, -2.9336, -2.9346, -3.0988, -3.8865, -3.5365, -4.2267, -3.6985, -2.0024, -3.8034, -6.0517, -7.2159, -7.9662, -8.7046, -8.9093, -9.5976, -9.6257, -9.1027, -9.6339, -9.9338, -9.9338, -9.8781, -9.9338, -9.9338, -9.9338, -9.9338, -9.8848, -9.7453, -9.4209, -9.8531, -9.5355, -9.5782, -9.6016, -9.7323, -9.6513, -9.3933, -2.1588, -2.0856, -2.473, -2.9322, -3.1722, -3.1461, -2.3398, -2.8782, -3.6225, -3.3557, -1.3798, -3.4315, -4.2961, -4.7165, -4.4438, -8.2351, -8.2725, -8.2159, -6.6849, -8.9195, -9.0366, -9.809, -9.7692, -9.9099, -10.0222, -9.9601, -9.6873, -10.0222, -9.9496, -10.0222, -9.8475, -9.8322, -9.5011, -8.3522, -9.6967, -9.6977, -9.6205, -2.0388, -2.4015, -2.4334, -1.2763, -3.1762, -2.6882, -3.2847, -2.6952, -3.937, -3.7745, -3.9371, -2.8227, -4.8566, -5.1636, -4.5287, -3.1656, -6.2341, -6.5046, -7.7084, -6.7946, -8.0749, -8.1621, -8.403, -8.7798, -8.7043, -9.072, -8.5746, -9.2197, -9.4652, -9.4087, -9.2311, -8.9897, -2.2672, -2.646, -2.6461, -2.726, -1.8959, -2.2175, -2.6535, -3.118, -2.4792, -3.9913, -3.8229, -4.1091, -3.8238, -4.5134, -4.5144, -2.3694, -5.2017, -5.0219, -3.9406, -7.8612, -6.3855, -8.7497, -8.5795, -9.2496, -9.6605, -9.4753, -9.8167, -9.8167, -9.7677, -9.6125, -9.2398, -8.8045, -9.623, -9.3593, -9.4753, -9.3655, -9.3473, -2.4329, -2.6439, -2.5871, -2.7047, -2.9122, -2.706, -2.3885, -2.9282, -2.7171, -3.007, -3.3972, -3.54, -3.4034, -3.7225, -3.3105, -3.8663, -3.6859, -4.5365, -3.9897, -4.7165, -4.8051, -5.227, -2.9323, -5.1737, -7.0069, -8.9127, -7.6598, -9.7258, -9.0253, -10.0337, -9.4818, -9.2986, -2.366, -2.9713, -3.0585, -2.819, -2.893, -2.8276, -2.9285, -1.024, -2.6893, -3.61, -3.3766, -4.4264, -5.2171, -2.8936, -5.1714, -4.9606, -6.0057, -8.1637, -8.8457, -7.5425, -8.6337, -9.2958, -9.8794, -9.5077, -9.8637, -9.9662, -10.0619, -10.0619, -10.0128, -10.0619, -9.8551, -8.5082, -9.8843, -9.6758, -9.8083, -9.8212, -9.8413, -1.1425, -2.3454, -2.7508, -2.8438, -2.9421, -2.6216, -2.4412, -3.1577, -2.8706, -3.0292, -4.0363, -3.5867, -4.3124, -4.7139, -3.6132, -5.6046, -8.7152, -8.9251, -8.8297, -9.155, -7.3553, -9.7368, -9.8413, -9.8413, -9.7923, -9.7923, -9.8413, -9.8413, -9.2921, -9.6735, -9.1537, -9.3978, -9.6794, -9.3765, -8.6796, -9.701, -9.5216, -9.615, -9.5574, -9.6499, -9.6496]}, \"token.table\": {\"Topic\": [6, 1, 3, 8, 2, 8, 8, 5, 6, 7, 5, 1, 6, 1, 1, 4, 2, 6, 7, 8, 4, 5, 6, 6, 5, 7, 4, 5, 7, 7, 3, 5, 6, 8, 1, 2, 3, 5, 6, 4, 6, 7, 7, 2, 4, 5, 7, 7, 3, 1, 1, 6, 6, 8, 5, 1, 5, 1, 3, 4, 2, 4, 5, 3, 6, 1, 4, 7, 8, 4, 6, 8, 2, 3, 5, 6, 3, 4, 6, 1, 2, 1, 6, 3, 4, 5, 6, 7, 5, 3, 7, 2, 3, 8, 3, 6, 1, 5, 8, 2, 5, 2, 2, 8, 5, 6, 3, 6, 2, 4, 8, 7, 1, 3, 6, 4, 6, 2, 4, 5, 6, 7, 7, 4, 8, 1, 4, 5, 8, 1, 7, 6, 8, 8, 3, 7], \"Freq\": [0.9740429875519645, 0.20033370821226779, 0.6010011246368033, 0.20033370821226779, 1.002217639616299, 0.9793762708104105, 1.000954357457227, 0.6586319691990835, 0.16465799229977088, 0.08232899614988544, 0.9617080946411894, 0.25548262787302006, 0.7025772266508051, 1.0175280639319209, 0.05551298603009684, 0.8882077764815495, 0.41222513716149606, 0.5152814214518701, 0.10958925889109498, 0.9315087005743073, 0.9853620942799886, 0.9621826640626733, 0.9903379993168334, 0.9946702923136514, 0.7388719270357156, 0.2814750198231297, 0.12429221158664573, 0.8700454811065201, 1.0340731319096867, 1.03471959650346, 0.20796919835381492, 0.10398459917690746, 0.7278921942383522, 0.9781982551119659, 0.9803441747987325, 1.0027789883245026, 0.5825395997129731, 0.29126979985648654, 0.14563489992824327, 0.7717883555922678, 0.09647354444903347, 0.09647354444903347, 0.9616522620473894, 0.23340437562543775, 0.06668696446441079, 0.033343482232205394, 0.6752055152021592, 0.9614487131670754, 1.0037362588212768, 1.0008218544458518, 0.1930650418242663, 0.7722601672970651, 0.7414398684983404, 0.24714662283278013, 0.9621905942770586, 0.8439327326648648, 0.18754060725885885, 0.23631915743167722, 0.7089574722950317, 0.0675197592661935, 1.0036313079905135, 0.6629317887884522, 0.37881816502197263, 0.45926742784123276, 0.5248770603899803, 0.39600124509318557, 0.6435020232764265, 0.6104743254749135, 0.3662845952849481, 0.3632260359250592, 0.454032544906324, 0.2724195269437944, 0.1840331515339944, 0.11041989092039664, 0.588906084908782, 0.1472265212271955, 0.9532776178693938, 0.1598671567778585, 0.8393025730837572, 0.6662821937654271, 0.28554951161375447, 0.7024340722921388, 0.32420034105791024, 0.5027620706787979, 0.10241449587901438, 0.1675873568929326, 0.11172490459528842, 0.11172490459528842, 1.0147413187199805, 0.6573739129459434, 0.3286869564729717, 1.0031214476950552, 0.7091455388698591, 0.23638184628995304, 1.0067454944117074, 0.9904704795824127, 1.0111982040979948, 0.19489076493652574, 0.779563059746103, 0.2708561408935806, 0.7042259663233097, 0.9964736850797449, 0.8995961118949075, 0.09995512354387862, 0.28006593805867924, 0.7468425014898112, 0.987773573200098, 0.9983565669091452, 0.32099837050742336, 0.6954964694327506, 0.9785413123731992, 0.9545357799027722, 0.6939016127135873, 0.20817048381407618, 0.13878032254271747, 1.0018670812062007, 1.0074365116894954, 1.001810764592232, 0.07937513604884293, 0.07937513604884293, 0.5556259523419005, 0.23812540814652877, 0.9907787947647589, 0.8983444864274907, 0.08637927754110487, 0.9933436440216813, 0.06082218801298677, 0.24328875205194708, 0.6690440681428546, 1.0013504570293932, 0.9555111996092139, 0.06579797758736892, 0.921171686223165, 0.9815089816060301, 0.9343973332222862, 0.10382192591358737], \"Term\": [\"amaze\", \"amazing\", \"amazing\", \"amazing\", \"atmosphere\", \"awesome\", \"back\", \"bland\", \"bland\", \"bland\", \"buffet\", \"burger\", \"burger\", \"cant\", \"chicken\", \"chicken\", \"cook\", \"cook\", \"definitely\", \"definitely\", \"delicious\", \"didnt\", \"disappointed\", \"dish\", \"dont\", \"dont\", \"eat\", \"eat\", \"enjoy\", \"enough\", \"everything\", \"everything\", \"everything\", \"excellent\", \"experience\", \"fantastic\", \"feel\", \"feel\", \"feel\", \"find\", \"find\", \"find\", \"flavor\", \"food\", \"food\", \"food\", \"food\", \"fresh\", \"friendly\", \"fry\", \"give\", \"give\", \"im\", \"im\", \"know\", \"leave\", \"leave\", \"love\", \"love\", \"love\", \"meal\", \"meat\", \"meat\", \"menu\", \"menu\", \"minute\", \"minute\", \"never\", \"never\", \"next\", \"next\", \"next\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"order\", \"order\", \"perfect\", \"perfect\", \"pizza\", \"pizza\", \"place\", \"place\", \"place\", \"place\", \"place\", \"pretty\", \"price\", \"price\", \"quality\", \"recommend\", \"recommend\", \"restaurant\", \"salad\", \"selection\", \"serve\", \"serve\", \"server\", \"server\", \"service\", \"side\", \"side\", \"slow\", \"slow\", \"staff\", \"star\", \"steak\", \"steak\", \"still\", \"sushi\", \"table\", \"table\", \"table\", \"take\", \"taste\", \"tasty\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"time\", \"time\", \"try\", \"vega\", \"vega\", \"vega\", \"wait\", \"wasnt\", \"way\", \"way\", \"wont\", \"worth\", \"worth\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el9267112477245016521021410\", ldavis_el9267112477245016521021410_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el9267112477245016521021410\", ldavis_el9267112477245016521021410_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el9267112477245016521021410\", ldavis_el9267112477245016521021410_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_model_1, bow_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saliency**: a measure of how much the term tells you about the topic.\n",
    "\n",
    "**Relevance**: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\n",
    "\n",
    "The **size** of the bubble measures the importance of the topics, relative to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?remove_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (7) Test on new text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.017241379310344827),\n",
       " (1, 0.017244333198368392),\n",
       " (2, 0.18971822952964995),\n",
       " (3, 0.16380888109767477),\n",
       " (4, 0.017241757713752862),\n",
       " (5, 0.017241379310344827),\n",
       " (6, 0.1775816068421047),\n",
       " (7, 0.3999224329977597)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try on a new (invented) text\n",
    "text_test = ['time', 'waste', 'food', 'staff', 'back', 'definitely']\n",
    "text_test_bow = dictionary.doc2bow(text_test)\n",
    "\n",
    "lda_model_1.get_document_topics(bow = text_test_bow)\n",
    "list(lda_model_1[text_test_bow])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe of topic probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use ```topicmod.lda_dtm2df``` to create a dataframe in which:\n",
    "- each row represents a document/text\n",
    "- has one column for each topic containing the probability of that topic for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DTM_df = lda_dtm2df(lda_model_1[bow_corpus], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0_lda</th>\n",
       "      <th>t1_lda</th>\n",
       "      <th>t2_lda</th>\n",
       "      <th>t3_lda</th>\n",
       "      <th>t4_lda</th>\n",
       "      <th>t5_lda</th>\n",
       "      <th>t6_lda</th>\n",
       "      <th>t7_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.611085</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.444581</td>\n",
       "      <td>0.035715</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.341126</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t0_lda    t1_lda    t2_lda    t3_lda    t4_lda    t5_lda    t6_lda  \\\n",
       "0  0.125000  0.125000  0.125000  0.125000  0.125000  0.125000  0.125000   \n",
       "1  0.055556  0.055556  0.055556  0.055556  0.055556  0.055556  0.611111   \n",
       "2  0.055556  0.055556  0.611085  0.055561  0.055565  0.055561  0.055561   \n",
       "3  0.035714  0.035721  0.035714  0.444581  0.035715  0.035714  0.341126   \n",
       "4  0.125000  0.125000  0.125000  0.125000  0.125000  0.125000  0.125000   \n",
       "\n",
       "     t7_lda  \n",
       "0  0.125000  \n",
       "1  0.055556  \n",
       "2  0.055556  \n",
       "3  0.035714  \n",
       "4  0.125000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe of topics' top n words and their probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use ```topicmod.lda_topic_top_words``` to create a dataframe in which:\n",
    "- each row represents a document/text\n",
    "- columns contains a list of that topic's top n words and a list of their the probability for that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0_top_words_lda</th>\n",
       "      <th>t1_top_words_lda</th>\n",
       "      <th>t2_top_words_lda</th>\n",
       "      <th>t3_top_words_lda</th>\n",
       "      <th>t4_top_words_lda</th>\n",
       "      <th>t5_top_words_lda</th>\n",
       "      <th>t6_top_words_lda</th>\n",
       "      <th>t7_top_words_lda</th>\n",
       "      <th>t0_top_word_pbs_lda</th>\n",
       "      <th>t1_top_word_pbs_lda</th>\n",
       "      <th>t2_top_word_pbs_lda</th>\n",
       "      <th>t3_top_word_pbs_lda</th>\n",
       "      <th>t4_top_word_pbs_lda</th>\n",
       "      <th>t5_top_word_pbs_lda</th>\n",
       "      <th>t6_top_word_pbs_lda</th>\n",
       "      <th>t7_top_word_pbs_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[wait, experience, try, fry, pizza, selection]</td>\n",
       "      <td>[service, food, meal, quality, fantastic, tasty]</td>\n",
       "      <td>[place, restaurant, friendly, love, staff, price]</td>\n",
       "      <td>[time, delicious, take, chicken, steak, minute]</td>\n",
       "      <td>[eat, dont, pretty, place, nice, didnt]</td>\n",
       "      <td>[order, taste, star, dish, salad, disappointed]</td>\n",
       "      <td>[food, think, never, flavor, fresh, wasnt]</td>\n",
       "      <td>[back, wont, definitely, way, awesome, still]</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wait, experience, try, fry, pizza, selection]</td>\n",
       "      <td>[service, food, meal, quality, fantastic, tasty]</td>\n",
       "      <td>[place, restaurant, friendly, love, staff, price]</td>\n",
       "      <td>[time, delicious, take, chicken, steak, minute]</td>\n",
       "      <td>[eat, dont, pretty, place, nice, didnt]</td>\n",
       "      <td>[order, taste, star, dish, salad, disappointed]</td>\n",
       "      <td>[food, think, never, flavor, fresh, wasnt]</td>\n",
       "      <td>[back, wont, definitely, way, awesome, still]</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[wait, experience, try, fry, pizza, selection]</td>\n",
       "      <td>[service, food, meal, quality, fantastic, tasty]</td>\n",
       "      <td>[place, restaurant, friendly, love, staff, price]</td>\n",
       "      <td>[time, delicious, take, chicken, steak, minute]</td>\n",
       "      <td>[eat, dont, pretty, place, nice, didnt]</td>\n",
       "      <td>[order, taste, star, dish, salad, disappointed]</td>\n",
       "      <td>[food, think, never, flavor, fresh, wasnt]</td>\n",
       "      <td>[back, wont, definitely, way, awesome, still]</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wait, experience, try, fry, pizza, selection]</td>\n",
       "      <td>[service, food, meal, quality, fantastic, tasty]</td>\n",
       "      <td>[place, restaurant, friendly, love, staff, price]</td>\n",
       "      <td>[time, delicious, take, chicken, steak, minute]</td>\n",
       "      <td>[eat, dont, pretty, place, nice, didnt]</td>\n",
       "      <td>[order, taste, star, dish, salad, disappointed]</td>\n",
       "      <td>[food, think, never, flavor, fresh, wasnt]</td>\n",
       "      <td>[back, wont, definitely, way, awesome, still]</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[wait, experience, try, fry, pizza, selection]</td>\n",
       "      <td>[service, food, meal, quality, fantastic, tasty]</td>\n",
       "      <td>[place, restaurant, friendly, love, staff, price]</td>\n",
       "      <td>[time, delicious, take, chicken, steak, minute]</td>\n",
       "      <td>[eat, dont, pretty, place, nice, didnt]</td>\n",
       "      <td>[order, taste, star, dish, salad, disappointed]</td>\n",
       "      <td>[food, think, never, flavor, fresh, wasnt]</td>\n",
       "      <td>[back, wont, definitely, way, awesome, still]</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 t0_top_words_lda  \\\n",
       "0  [wait, experience, try, fry, pizza, selection]   \n",
       "1  [wait, experience, try, fry, pizza, selection]   \n",
       "2  [wait, experience, try, fry, pizza, selection]   \n",
       "3  [wait, experience, try, fry, pizza, selection]   \n",
       "4  [wait, experience, try, fry, pizza, selection]   \n",
       "\n",
       "                                   t1_top_words_lda  \\\n",
       "0  [service, food, meal, quality, fantastic, tasty]   \n",
       "1  [service, food, meal, quality, fantastic, tasty]   \n",
       "2  [service, food, meal, quality, fantastic, tasty]   \n",
       "3  [service, food, meal, quality, fantastic, tasty]   \n",
       "4  [service, food, meal, quality, fantastic, tasty]   \n",
       "\n",
       "                                    t2_top_words_lda  \\\n",
       "0  [place, restaurant, friendly, love, staff, price]   \n",
       "1  [place, restaurant, friendly, love, staff, price]   \n",
       "2  [place, restaurant, friendly, love, staff, price]   \n",
       "3  [place, restaurant, friendly, love, staff, price]   \n",
       "4  [place, restaurant, friendly, love, staff, price]   \n",
       "\n",
       "                                  t3_top_words_lda  \\\n",
       "0  [time, delicious, take, chicken, steak, minute]   \n",
       "1  [time, delicious, take, chicken, steak, minute]   \n",
       "2  [time, delicious, take, chicken, steak, minute]   \n",
       "3  [time, delicious, take, chicken, steak, minute]   \n",
       "4  [time, delicious, take, chicken, steak, minute]   \n",
       "\n",
       "                          t4_top_words_lda  \\\n",
       "0  [eat, dont, pretty, place, nice, didnt]   \n",
       "1  [eat, dont, pretty, place, nice, didnt]   \n",
       "2  [eat, dont, pretty, place, nice, didnt]   \n",
       "3  [eat, dont, pretty, place, nice, didnt]   \n",
       "4  [eat, dont, pretty, place, nice, didnt]   \n",
       "\n",
       "                                  t5_top_words_lda  \\\n",
       "0  [order, taste, star, dish, salad, disappointed]   \n",
       "1  [order, taste, star, dish, salad, disappointed]   \n",
       "2  [order, taste, star, dish, salad, disappointed]   \n",
       "3  [order, taste, star, dish, salad, disappointed]   \n",
       "4  [order, taste, star, dish, salad, disappointed]   \n",
       "\n",
       "                             t6_top_words_lda  \\\n",
       "0  [food, think, never, flavor, fresh, wasnt]   \n",
       "1  [food, think, never, flavor, fresh, wasnt]   \n",
       "2  [food, think, never, flavor, fresh, wasnt]   \n",
       "3  [food, think, never, flavor, fresh, wasnt]   \n",
       "4  [food, think, never, flavor, fresh, wasnt]   \n",
       "\n",
       "                                t7_top_words_lda  \\\n",
       "0  [back, wont, definitely, way, awesome, still]   \n",
       "1  [back, wont, definitely, way, awesome, still]   \n",
       "2  [back, wont, definitely, way, awesome, still]   \n",
       "3  [back, wont, definitely, way, awesome, still]   \n",
       "4  [back, wont, definitely, way, awesome, still]   \n",
       "\n",
       "                                 t0_top_word_pbs_lda  \\\n",
       "0  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "1  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "2  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "3  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "4  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "\n",
       "                                 t1_top_word_pbs_lda  \\\n",
       "0  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "1  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "2  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "3  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "4  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "\n",
       "                                 t2_top_word_pbs_lda  \\\n",
       "0  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "1  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "2  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "3  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "4  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "\n",
       "                                 t3_top_word_pbs_lda  \\\n",
       "0  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "1  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "2  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "3  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "4  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "\n",
       "                                 t4_top_word_pbs_lda  \\\n",
       "0  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "1  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "2  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "3  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "4  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "\n",
       "                                 t5_top_word_pbs_lda  \\\n",
       "0  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "1  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "2  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "3  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "4  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "\n",
       "                                 t6_top_word_pbs_lda  \\\n",
       "0  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "1  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "2  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "3  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "4  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "\n",
       "                                 t7_top_word_pbs_lda  \n",
       "0  [0.3190092414244476, 0.09580831450158118, 0.08...  \n",
       "1  [0.3190092414244476, 0.09580831450158118, 0.08...  \n",
       "2  [0.3190092414244476, 0.09580831450158118, 0.08...  \n",
       "3  [0.3190092414244476, 0.09580831450158118, 0.08...  \n",
       "4  [0.3190092414244476, 0.09580831450158118, 0.08...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (iv) extract top n words and their probabilities for each topic, turn them into a dataframe\n",
    "\n",
    "words_topics_dict = lda_topic_top_words(lda_mod = lda_model_1, n_top_words = 6)\n",
    "words_topics_df = topictopwords_dict2df(words_topics_dict, orig_dataset = df, tech = 'lda')\n",
    "\n",
    "words_topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with top n topics for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use ```topicmod.lda_ranked_topics2df``` to create a dataframe in which:\n",
    "- rows are documents\n",
    "- the first column ```ranked_topics_lda``` contains tuples of ranked topics for each document; if all topics have the same probability for a document, then topics are ordered by their index (e.g., (0,1,2,3))\n",
    "- the second column ```ranked_topics_pbs_lda``` contains tuples of the ranked topics' probabilities for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranked_topics_lda</th>\n",
       "      <th>ranked_topics_pbs_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7)</td>\n",
       "      <td>(0.12500000000000003, 0.12500000000000003, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(6, 0, 1, 2, 3, 4, 5, 7)</td>\n",
       "      <td>(0.611111111111111, 0.055555555555555566, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2, 4, 3, 6, 5, 0, 1, 7)</td>\n",
       "      <td>(0.6110855498655305, 0.05556479353724922, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3, 6, 1, 4, 0, 2, 5, 7)</td>\n",
       "      <td>(0.4446544538063567, 0.3410524768197033, 0.035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7)</td>\n",
       "      <td>(0.12500000000000003, 0.12500000000000003, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ranked_topics_lda                              ranked_topics_pbs_lda\n",
       "0  (0, 1, 2, 3, 4, 5, 6, 7)  (0.12500000000000003, 0.12500000000000003, 0.1...\n",
       "1  (6, 0, 1, 2, 3, 4, 5, 7)  (0.611111111111111, 0.055555555555555566, 0.05...\n",
       "2  (2, 4, 3, 6, 5, 0, 1, 7)  (0.6110855498655305, 0.05556479353724922, 0.05...\n",
       "3  (3, 6, 1, 4, 0, 2, 5, 7)  (0.4446544538063567, 0.3410524768197033, 0.035...\n",
       "4  (0, 1, 2, 3, 4, 5, 6, 7)  (0.12500000000000003, 0.12500000000000003, 0.1..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (v) Ranked topics for each doc, create a DTM dataframe \n",
    "\n",
    "ranked_DTM_df =  lda_ranked_topics2df(lda_mod = lda_model_1, corpus = bow_corpus)\n",
    "\n",
    "ranked_DTM_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join LDA-results datasets with the original dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll be using our ```nlpfunctions.utils.merge_dfs``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_df = merge_dfs(df, DTM_df, words_topics_df, ranked_DTM_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>source</th>\n",
       "      <th>text_lemmas</th>\n",
       "      <th>t0_lda</th>\n",
       "      <th>t1_lda</th>\n",
       "      <th>t2_lda</th>\n",
       "      <th>t3_lda</th>\n",
       "      <th>t4_lda</th>\n",
       "      <th>t5_lda</th>\n",
       "      <th>...</th>\n",
       "      <th>t0_top_word_pbs_lda</th>\n",
       "      <th>t1_top_word_pbs_lda</th>\n",
       "      <th>t2_top_word_pbs_lda</th>\n",
       "      <th>t3_top_word_pbs_lda</th>\n",
       "      <th>t4_top_word_pbs_lda</th>\n",
       "      <th>t5_top_word_pbs_lda</th>\n",
       "      <th>t6_top_word_pbs_lda</th>\n",
       "      <th>t7_top_word_pbs_lda</th>\n",
       "      <th>ranked_topics_lda</th>\n",
       "      <th>ranked_topics_pbs_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "      <td>[wow, love, place]</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7)</td>\n",
       "      <td>(0.12500000000000003, 0.12500000000000003, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>[crust]</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "      <td>(6, 0, 1, 2, 3, 4, 5, 7)</td>\n",
       "      <td>(0.611111111111111, 0.055555555555555566, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "      <td>[tasty, texture, nasty]</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.611085</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "      <td>(2, 4, 3, 6, 5, 0, 1, 7)</td>\n",
       "      <td>(0.6110855498655305, 0.05556479353724922, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "      <td>[stop, bank, holiday, rick, steve, recommendat...</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.444581</td>\n",
       "      <td>0.035715</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "      <td>(3, 6, 1, 4, 0, 2, 5, 7)</td>\n",
       "      <td>(0.4446544538063567, 0.3410524768197033, 0.035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "      <td>[selection, menu, price]</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.17612636239374957, 0.12839652586101735, 0.0...</td>\n",
       "      <td>[0.406189714403089, 0.1350157245082152, 0.0679...</td>\n",
       "      <td>[0.25163471462447395, 0.12423143385448057, 0.1...</td>\n",
       "      <td>[0.2790633107606138, 0.13019022058036264, 0.09...</td>\n",
       "      <td>[0.1501842578547173, 0.10888185770232081, 0.10...</td>\n",
       "      <td>[0.09177112607489511, 0.0877817140426222, 0.07...</td>\n",
       "      <td>[0.35917209078662504, 0.09385083878293446, 0.0...</td>\n",
       "      <td>[0.3190092414244476, 0.09580831450158118, 0.08...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7)</td>\n",
       "      <td>(0.12500000000000003, 0.12500000000000003, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score source  \\\n",
       "0                           Wow... Loved this place.      1   yelp   \n",
       "1                                 Crust is not good.      0   yelp   \n",
       "2          Not tasty and the texture was just nasty.      0   yelp   \n",
       "3  Stopped by during the late May bank holiday of...      1   yelp   \n",
       "4  The selection on the menu was great and so wer...      1   yelp   \n",
       "\n",
       "                                         text_lemmas    t0_lda    t1_lda  \\\n",
       "0                                 [wow, love, place]  0.125000  0.125000   \n",
       "1                                            [crust]  0.055556  0.055556   \n",
       "2                            [tasty, texture, nasty]  0.055556  0.055556   \n",
       "3  [stop, bank, holiday, rick, steve, recommendat...  0.035714  0.035721   \n",
       "4                           [selection, menu, price]  0.125000  0.125000   \n",
       "\n",
       "     t2_lda    t3_lda    t4_lda    t5_lda  \\\n",
       "0  0.125000  0.125000  0.125000  0.125000   \n",
       "1  0.055556  0.055556  0.055556  0.055556   \n",
       "2  0.611085  0.055561  0.055565  0.055561   \n",
       "3  0.035714  0.444581  0.035715  0.035714   \n",
       "4  0.125000  0.125000  0.125000  0.125000   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                 t0_top_word_pbs_lda  \\\n",
       "0  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "1  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "2  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "3  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "4  [0.17612636239374957, 0.12839652586101735, 0.0...   \n",
       "\n",
       "                                 t1_top_word_pbs_lda  \\\n",
       "0  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "1  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "2  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "3  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "4  [0.406189714403089, 0.1350157245082152, 0.0679...   \n",
       "\n",
       "                                 t2_top_word_pbs_lda  \\\n",
       "0  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "1  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "2  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "3  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "4  [0.25163471462447395, 0.12423143385448057, 0.1...   \n",
       "\n",
       "                                 t3_top_word_pbs_lda  \\\n",
       "0  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "1  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "2  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "3  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "4  [0.2790633107606138, 0.13019022058036264, 0.09...   \n",
       "\n",
       "                                 t4_top_word_pbs_lda  \\\n",
       "0  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "1  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "2  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "3  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "4  [0.1501842578547173, 0.10888185770232081, 0.10...   \n",
       "\n",
       "                                 t5_top_word_pbs_lda  \\\n",
       "0  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "1  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "2  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "3  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "4  [0.09177112607489511, 0.0877817140426222, 0.07...   \n",
       "\n",
       "                                 t6_top_word_pbs_lda  \\\n",
       "0  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "1  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "2  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "3  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "4  [0.35917209078662504, 0.09385083878293446, 0.0...   \n",
       "\n",
       "                                 t7_top_word_pbs_lda  \\\n",
       "0  [0.3190092414244476, 0.09580831450158118, 0.08...   \n",
       "1  [0.3190092414244476, 0.09580831450158118, 0.08...   \n",
       "2  [0.3190092414244476, 0.09580831450158118, 0.08...   \n",
       "3  [0.3190092414244476, 0.09580831450158118, 0.08...   \n",
       "4  [0.3190092414244476, 0.09580831450158118, 0.08...   \n",
       "\n",
       "          ranked_topics_lda                              ranked_topics_pbs_lda  \n",
       "0  (0, 1, 2, 3, 4, 5, 6, 7)  (0.12500000000000003, 0.12500000000000003, 0.1...  \n",
       "1  (6, 0, 1, 2, 3, 4, 5, 7)  (0.611111111111111, 0.055555555555555566, 0.05...  \n",
       "2  (2, 4, 3, 6, 5, 0, 1, 7)  (0.6110855498655305, 0.05556479353724922, 0.05...  \n",
       "3  (3, 6, 1, 4, 0, 2, 5, 7)  (0.4446544538063567, 0.3410524768197033, 0.035...  \n",
       "4  (0, 1, 2, 3, 4, 5, 6, 7)  (0.12500000000000003, 0.12500000000000003, 0.1...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the LDA results as input for other analyses. For instance, here we show how we could include documents' ranked topics and probabilities in a classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lda_topic_pbs = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['t0_lda', 't1_lda', 't2_lda', 't3_lda', 't4_lda', 't5_lda', 't6_lda', 't7_lda']))\n",
    "        \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vec = CountVectorizer(tokenizer = word_tokenize,\n",
    "                         analyzer=\"word\",\n",
    "                         ngram_range = (1,3),\n",
    "                         stop_words=None,\n",
    "                         min_df=1\n",
    "                         )\n",
    "\n",
    "pipe_bags_words = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['text'])),\n",
    "        ('transformer', Series2ListOfStrings()),\n",
    "        ('vec', my_vec),\n",
    "        ('tf_idf', TfidfTransformer())\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier\n",
    "\n",
    "svm = SVC(probability=True, C=1, kernel='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_bow_svm_clf = Pipeline([\n",
    "        \n",
    "        # Combined text (bag-of-word) and ad-hoc features\n",
    "        ('features', FeatureUnion(\n",
    "            \n",
    "                transformer_list = [\n",
    "                        ('lda', pipe_lda_topic_pbs),\n",
    "                        ('bow', pipe_bags_words)\n",
    "                        ],\n",
    "            \n",
    "                # weight components in FeatureUnion\n",
    "                transformer_weights={\n",
    "                        'lda': 0.6,\n",
    "                        'bow': 1.0\n",
    "                        }    \n",
    "                )),\n",
    "        \n",
    "        # Use classifier on combined features\n",
    "        ('classifier', svm)\n",
    "        \n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = text_df[['text', 't0_lda', 't1_lda', 't2_lda', 't3_lda', 't4_lda', 't5_lda', 't6_lda', 't7_lda']]\n",
    "y = text_df.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_svm = cross_validate(lda_bow_svm_clf, X, y, cv = 5, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.998 (std 0.001)\n",
      "Test Accuracy: 0.816 (std 0.021)\n"
     ]
    }
   ],
   "source": [
    "sorted(scores_svm.keys())\n",
    "\n",
    "print(\"Train Accuracy: %0.3f (std %0.3f)\" % (scores_svm['train_score'].mean(), scores_svm['train_score'].std()))\n",
    "print(\"Test Accuracy: %0.3f (std %0.3f)\" % (scores_svm['test_score'].mean(), scores_svm['test_score'].std()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TuplesToColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class for building sklearn Pipeline step. \n",
    "    This class turns columns from a pandas data frame (type Series) that\n",
    "    contain tuples of integer or float values into separate array columns \n",
    "    that can be used as inout in ML pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialise\n",
    "    def __init__(self):\n",
    "        self              #could also use 'pass'\n",
    "        \n",
    "    # \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):        #X: dataset to pass to the transformer.\n",
    "        cols_df = pd.DataFrame()\n",
    "        for name, valuez in X.iteritems():\n",
    "            valuez_df = pd.DataFrame(valuez.values.tolist())\n",
    "            cols_df = pd.concat([cols_df, valuez_df], axis=1)\n",
    "        return cols_df.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_lda_topic_pbs_2 = Pipeline([\n",
    "    \n",
    "    ('selector', ColumnSelector(columns=['ranked_topics_lda', 'ranked_topics_pbs_lda'])),\n",
    "    ('toarray', TuplesToColumns())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_bow_svm_clf_2 = Pipeline([\n",
    "        \n",
    "        # Combined text (bag-of-word) and ad-hoc features\n",
    "        ('features', FeatureUnion(\n",
    "            \n",
    "                transformer_list = [\n",
    "                        ('lda', pipe_lda_topic_pbs_2),\n",
    "                        ('bow', pipe_bags_words)\n",
    "                        ],\n",
    "            \n",
    "                # weight components in FeatureUnion\n",
    "                transformer_weights={\n",
    "                        'lda': 0.6,\n",
    "                        'bow': 1.0\n",
    "                        }    \n",
    "                )),\n",
    "        \n",
    "        # Use classifier on combined features\n",
    "        ('classifier', svm)\n",
    "        \n",
    "        ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = text_df[['text', 'ranked_topics_lda', 'ranked_topics_pbs_lda']]\n",
    "y = text_df.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_svm = cross_validate(lda_bow_svm_clf_2, X, y, cv = 5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.998 (std 0.001)\n",
      "Test Accuracy: 0.821 (std 0.029)\n"
     ]
    }
   ],
   "source": [
    "sorted(scores_svm.keys())\n",
    "\n",
    "print(\"Train Accuracy: %0.3f (std %0.3f)\" % (scores_svm['train_score'].mean(), scores_svm['train_score'].std()))\n",
    "print(\"Test Accuracy: %0.3f (std %0.3f)\" % (scores_svm['test_score'].mean(), scores_svm['test_score'].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttc = TuplesToColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 2.        , ..., 0.125     , 0.125     ,\n",
       "        0.125     ],\n",
       "       [6.        , 0.        , 1.        , ..., 0.05555556, 0.05555556,\n",
       "        0.05555556],\n",
       "       [2.        , 4.        , 3.        , ..., 0.05555556, 0.05555556,\n",
       "        0.05555556],\n",
       "       ...,\n",
       "       [7.        , 4.        , 0.        , ..., 0.05555556, 0.05555556,\n",
       "        0.05555556],\n",
       "       [1.        , 6.        , 3.        , ..., 0.03571429, 0.03571429,\n",
       "        0.03571429],\n",
       "       [5.        , 0.        , 1.        , ..., 0.05555556, 0.05555556,\n",
       "        0.05555556]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttc.fit_transform(text_df[['ranked_topics_lda', 'ranked_topics_pbs_lda']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
